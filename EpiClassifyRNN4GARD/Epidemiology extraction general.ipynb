{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "from termcolor import colored\n",
    "from spacy import displacy\n",
    "import nltk\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "st = StanfordNERTagger('stanford-ner/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
    "                        'stanford-ner/stanford-ner.jar',\n",
    "                        encoding='utf-8')\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "nlpSci = spacy.load(\"en_ner_bc5cdr_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epi_abs = pd.read_csv('epidemiology_classifications.csv', header=None, skiprows=[0],\n",
    "                        names=['is_epi','pmid','abs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printHighlighted(doc, indices):\n",
    "    final = ''\n",
    "    start = 0\n",
    "    for i in indices:\n",
    "        final += doc[start:i[0]].text+' '\n",
    "        final += colored(doc[i[0]:i[1]].text, 'red', 'on_yellow', attrs=['bold']) + ' '\n",
    "        start = i[1]\n",
    "    final += doc[start:].text\n",
    "    print(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeDuplicates(a):\n",
    "    for i in range(len(a)-1,0,-1):\n",
    "        if a[i] == a[i-1]:\n",
    "            del a[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOCATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLocsNltk(text):\n",
    "    tokenized_text = word_tokenize(text)\n",
    "    classified_text = st.tag(tokenized_text)\n",
    "    locs = set()\n",
    "\n",
    "    for word in classified_text:\n",
    "        if word[1] == 'LOCATION':\n",
    "            if word[0] not in locs:\n",
    "                locs.add(word[0])\n",
    "    \n",
    "    return locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLocsSpacy(doc):\n",
    "    locs = {}\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'GPE':\n",
    "            tokens = {token.text for token in ent}\n",
    "            if ent.text not in locs:\n",
    "                locs[ent.text] = tokens\n",
    "            else:\n",
    "                for t in tokens:\n",
    "                    if t not in locs[ent.text]:\n",
    "                        locs[ent.text].add(t)\n",
    "                \n",
    "    return locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLocs(text):\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    spacyLocs = getLocsSpacy(doc)\n",
    "    nltkLocs = getLocsNltk(text)\n",
    "    locs = []\n",
    "    \n",
    "    for entity in spacyLocs:\n",
    "        if len(spacyLocs[entity] & nltkLocs) != 0:\n",
    "            locs.append(entity)\n",
    "            \n",
    "    return locs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTokenChunkDict(doc):\n",
    "    chunks = [chunk for chunk in doc.noun_chunks]\n",
    "    tokenToChunk = {}\n",
    "    for chunk in chunks:\n",
    "        for i in range(chunk.start, chunk.end):\n",
    "            tokenToChunk[i] = [chunk.start, chunk.end]\n",
    "    return tokenToChunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isValidStat(token):\n",
    "    ancestors = {a.text.lower() for a in token.ancestors}\n",
    "    if 'ci' in ancestors or 'confidence' in ancestors or 'interval' in ancestors or 'p' in ancestors or 'p-value' in ancestors or 'type' in ancestors:\n",
    "        return False\n",
    "    if 'times' in ancestors:\n",
    "        return False\n",
    "    if token.text.lower() == 'one' and len(token.doc) > token.i + 1 and token.doc[token.i + 1].text == 'of':\n",
    "        return False\n",
    "    if token.ent_type_ == 'DATE':\n",
    "        return False\n",
    "    if token.ent_type_ in {'CARDINAL','QUANTITY'}:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getStats(abst, display=False):\n",
    "    doc = nlp(abst)\n",
    "    indices = []\n",
    "    tokenToChunk = getTokenChunkDict(doc)\n",
    "    key_val_dz = []\n",
    "    \n",
    "    for sent in doc.sents:\n",
    "        keywords = []\n",
    "        values = []\n",
    "        dzs = []\n",
    "        \n",
    "        keywords_text = []\n",
    "        values_text = []\n",
    "        dzs_text = []\n",
    "        \n",
    "        sciSent = nlpSci(sent.text)\n",
    "        \n",
    "        for token in sent:\n",
    "            sciToken = nlpSci(token.text)[0]\n",
    "            if token.text.lower() in {'prevalence','incidence','frequency','PR','prevalences','occurrence'}:\n",
    "                if token.i in tokenToChunk:\n",
    "                    keywords.append(tokenToChunk[token.i])\n",
    "                else:\n",
    "                    keywords.append([token.i, token.i+1])\n",
    "            if isValidStat(token) or isValidStat(nlp(token.text)[0]):\n",
    "                if token.i in tokenToChunk:\n",
    "                    values.append(tokenToChunk[token.i])\n",
    "                else:\n",
    "                    values.append([token.i, token.i+1])\n",
    "        if keywords != [] and values != []:\n",
    "            for token in sciSent:\n",
    "                if token.ent_type_ == 'DISEASE':\n",
    "                    for token_reg in sent:\n",
    "                        if token_reg.text == token.text:\n",
    "                            if token_reg.i in tokenToChunk:\n",
    "                                dzs.append(tokenToChunk[token_reg.i])\n",
    "                            else:\n",
    "                                dzs.append([token_reg.i, token_reg.i+1])\n",
    "            \n",
    "            removeDuplicates(keywords)\n",
    "            removeDuplicates(values)\n",
    "            removeDuplicates(dzs)\n",
    "            for i in keywords:\n",
    "                keywords_text.append(doc[i[0]:i[1]])\n",
    "            for i in values:\n",
    "                values_text.append(doc[i[0]:i[1]])\n",
    "            for i in dzs:\n",
    "                dzs_text.append(doc[i[0]:i[1]])\n",
    "            key_val_dz.append((keywords_text, values_text, dzs_text))\n",
    "            indices += keywords\n",
    "            indices += values\n",
    "            indices += dzs\n",
    "    indices = sorted(indices)\n",
    "    removeDuplicates(indices)\n",
    "    if display:\n",
    "        printHighlighted(doc, indices)\n",
    "    return key_val_dz\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'Incidence of the disease in Olmsted County, Minnesota, was 2.6/million/year.'\n",
    "doc = nlp(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getStats(sent, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INFORMATION EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,row in epi_abs.iterrows():\n",
    "    if row['is_epi'] == True:\n",
    "        locs = getLocs(row['abs'])\n",
    "        info = getStats(row['abs'], True)\n",
    "        print(locs)\n",
    "        print(info)\n",
    "        print(row['pmid'])\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(sent):\n",
    "    ## chunk 1\n",
    "    ent1 = \"\"\n",
    "    ent2 = \"\"\n",
    "\n",
    "    #############################################################\n",
    "\n",
    "    for tok in nlp(sent):\n",
    "        ## chunk 2\n",
    "        # if token is a punctuation mark then move on to the next token\n",
    "        if tok.dep_ != \"punct\":\n",
    "            ## chunk 3\n",
    "            if tok.dep_.find(\"subj\") == True:\n",
    "                ent1 = tok.text  \n",
    "\n",
    "            ## chunk 4\n",
    "            if tok.dep_.find(\"obj\") == True:\n",
    "                ent2 = tok.text\n",
    "\n",
    "        print('\\nent1:',ent1)\n",
    "        print('ent2:',ent2)\n",
    "    #############################################################\n",
    "\n",
    "    return [ent1.strip(), ent2.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardizeSent(sent):\n",
    "    doc = nlp(sent)\n",
    "    newSent = sent\n",
    "    for e in reversed(doc.ents):\n",
    "        if e.label_ in {'PERCENT','CARDINAL','GPE','LOC','DATE','TIME','QUANTITY','ORDINAL'}:\n",
    "            l = e.label_\n",
    "            if e.text[0].isdigit():\n",
    "                l = 'CARDINAL'\n",
    "            start = e.start_char\n",
    "            end = start + len(e.text)\n",
    "            newSent = newSent[:start] + l + newSent[end:]\n",
    "    return newSent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'Incidence of the disease in Olmsted County, Minnesota, was 2.6/million/year'\n",
    "sent = standardizeSent(sent)\n",
    "print(sent)\n",
    "doc = nlp(sent)\n",
    "print([chunk for chunk in doc.noun_chunks])\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_entities(sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

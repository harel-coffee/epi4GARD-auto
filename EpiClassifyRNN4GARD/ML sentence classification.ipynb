{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "from optparse import OptionParser\n",
    "import sys\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import pickle\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.utils.extmath import density\n",
    "from sklearn import metrics\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "nlpSci = spacy.load(\"en_ner_bc5cdr_md\")\n",
    "nlpSci2 = spacy.load('en_ner_bionlp13cg_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare sentence datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'epidemiology_classifications_sents.csv'\n",
    "df = pd.read_csv(filename, header=None, skiprows=None, names=['label','pmid','sent'])\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shuffled = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'epidemiology_classifications.csv'\n",
    "df_abs = pd.read_csv(filename, header=None, skiprows=[0], names=['label','pmid','abs'])\n",
    "df_abs.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import metrics\n",
    "import random\n",
    "import csv\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 5000\n",
    "embedding_dim = 64\n",
    "max_length = 200\n",
    "trunc_type = 'post'\n",
    "padding_type = 'post'\n",
    "oov_tok = '<OOV>'\n",
    "training_portion = .8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = []\n",
    "labels = []\n",
    "pmids = []\n",
    "\n",
    "with open(\"epidemiology_classifications_sents.csv\", 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        sent = row[2]\n",
    "        for word in STOPWORDS:\n",
    "            token = ' ' + word + ' '\n",
    "            sent = sent.replace(token, ' ')\n",
    "            sent = sent.replace(' ', ' ')\n",
    "        if len(sent)>5:\n",
    "            sents.append(sent)\n",
    "            labels.append(int(row[0] == 'True'))\n",
    "            pmids.append(row[1])\n",
    "\n",
    "combined = list(zip(labels, sents, pmids))\n",
    "random.shuffle(combined)\n",
    "labels, sents, pmids = zip(*combined)\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmid_to_indices = {}\n",
    "for i in range(len(pmids)):\n",
    "    pmid = pmids[i]\n",
    "    if pmid in pmid_to_indices:\n",
    "        pmid_to_indices[pmid].append(i)\n",
    "    else:\n",
    "        pmid_to_indices[pmid] = [i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num_abs = int(len(pmid_to_indices) * training_portion)\n",
    "pmid_list = list(pmid_to_indices.keys())\n",
    "train_pmid_list = pmid_list[0 : train_num_abs]\n",
    "validation_pmid_list = pmid_list[train_num_abs:]\n",
    "\n",
    "train_sents = []\n",
    "train_labels = []\n",
    "train_pmids = []\n",
    "validation_sents = []\n",
    "validation_labels = []\n",
    "validation_pmids = []\n",
    "\n",
    "for pmid in train_pmid_list:\n",
    "    for i in pmid_to_indices[pmid]:\n",
    "        train_sents.append(sents[i])\n",
    "        train_labels.append(labels[i])\n",
    "        train_pmids.append(pmid)\n",
    "        \n",
    "for pmid in validation_pmid_list:\n",
    "    for i in pmid_to_indices[pmid]:\n",
    "        validation_sents.append(sents[i])\n",
    "        validation_labels.append(labels[i])\n",
    "        validation_pmids.append(pmid)\n",
    "        \n",
    "combined = list(zip(train_sents, train_labels, train_pmids))\n",
    "random.shuffle(combined)\n",
    "train_sents, train_labels, train_pmids = zip(*combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_labels))\n",
    "print(len(validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_pmid_list), len(validation_pmid_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = 0\n",
    "neg = 0\n",
    "for l in validation_labels:\n",
    "    if l==1:\n",
    "        pos+=1\n",
    "    else:\n",
    "        neg +=1\n",
    "print(pos,neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardizeSent(sent):\n",
    "    doc = nlp(sent)\n",
    "    newSent = sent\n",
    "    for e in reversed(doc.ents):\n",
    "        if e.label_ in {'PERCENT','CARDINAL','GPE','LOC','DATE','TIME','QUANTITY','ORDINAL'}:\n",
    "            start = e.start_char\n",
    "            end = start + len(e.text)\n",
    "            newSent = newSent[:start] + e.label_ + newSent[end:]\n",
    "    return newSent\n",
    "\n",
    "def standardizeSciTerms(sent):\n",
    "    doc = nlpSci(sent)\n",
    "    newSent = sent\n",
    "    for e in reversed(doc.ents):\n",
    "        start = e.start_char\n",
    "        end = start + len(e.text)\n",
    "        newSent = newSent[:start] + e.label_ + newSent[end:]\n",
    "        \n",
    "    doc = nlpSci2(newSent)\n",
    "    for e in reversed(doc.ents):\n",
    "        start = e.start_char\n",
    "        end = start + len(e.text)\n",
    "        newSent = newSent[:start] + e.label_ + newSent[end:]\n",
    "    return newSent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sents_standard = [standardizeSent(standardizeSciTerms(sent)) for sent in train_sents]\n",
    "val_sents_standard = [standardizeSent(standardizeSciTerms(sent)) for sent in validation_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(train_sents_standard)\n",
    "word_index = tokenizer.word_index\n",
    "dict(list(word_index.items())[0:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = tokenizer.texts_to_sequences(train_sents_standard)\n",
    "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_sequences = tokenizer.texts_to_sequences(val_sents_standard)\n",
    "validation_padded = pad_sequences(validation_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "print(len(val_sents_standard))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_label_seq = np.array(train_labels) #np.array(label_tokenizer.texts_to_sequences(train_labels))\n",
    "validation_label_seq = np.array(validation_labels) #np.array(label_tokenizer.texts_to_sequences(validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "def decode_sent(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
    "print(decode_sent(train_padded[1]))\n",
    "print('---')\n",
    "print(train_sents[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim)),\n",
    "    tf.keras.layers.Dense(embedding_dim, activation='relu'),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam'\n",
    "              , metrics=['accuracy'])\n",
    "num_epochs = 10\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
    "history = model.fit(train_padded, training_label_seq, epochs=num_epochs, validation_data=(validation_padded, validation_label_seq), verbose=2, callbacks=[es])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p saved_model\n",
    "model.save('saved_model/my_model') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "y_pred1 = model.predict(validation_padded)\n",
    "y_pred = np.argmax(y_pred1, axis=1)\n",
    "\n",
    "print(precision_score(validation_label_seq, y_pred , average=\"macro\"))\n",
    "print(recall_score(validation_label_seq, y_pred , average=\"macro\"))\n",
    "print(f1_score(validation_label_seq, y_pred , average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = tf.keras.metrics.AUC()\n",
    "_ = m.update_state(validation_label_seq, y_pred)\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmid_to_indices_val = {}\n",
    "for i in range(len(validation_pmids)):\n",
    "    pmid = validation_pmids[i]\n",
    "    if pmid in pmid_to_indices_val:\n",
    "        pmid_to_indices_val[pmid].append(i)\n",
    "    else:\n",
    "        pmid_to_indices_val[pmid] = [i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    if y_pred[i]==0 and validation_label_seq[i]==1:\n",
    "        print('\\nprediction:',y_pred[i], y_pred1[i])\n",
    "        print('label:',validation_label_seq[i])\n",
    "        print(validation_sents[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pmid in pmid_to_indices_val:\n",
    "    print('\\n',pmid)\n",
    "    epi_count = 0\n",
    "    print(df_abs.loc[df_abs['pmid'] == int(pmid)]['abs'].item())\n",
    "    for i in pmid_to_indices_val[pmid]:\n",
    "        if y_pred[i] == 1:\n",
    "            epi_count +=1\n",
    "        print('\\nprediction:',y_pred[i], y_pred1[i])\n",
    "        print('label:',validation_label_seq[i])\n",
    "        print(validation_sents[i])\n",
    "    print('NUMBER OF EPI SENTS:',epi_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on one example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmid = 26795590\n",
    "url = 'https://www.ebi.ac.uk/europepmc/webservices/rest/search?query=EXT_ID:'+str(pmid)+'&resulttype=core'\n",
    "r = requests.get(url)\n",
    "root = ET.fromstring(r.content)\n",
    "new_model = tf.keras.models.load_model('saved_model/my_model')\n",
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    new_tokenizer = pickle.load(handle)\n",
    "abstract = ''\n",
    "isEpi = False\n",
    "for child in root.iter('*'):\n",
    "    if child.tag == 'abstractText':\n",
    "        abstract = child.text\n",
    "doc = nlp(abstract)\n",
    "sents = [sent.text for sent in doc.sents]\n",
    "\n",
    "for sent in sents:\n",
    "    for word in STOPWORDS:\n",
    "            token = ' ' + word + ' '\n",
    "            sent = sent.replace(token, ' ')\n",
    "            sent = sent.replace(' ', ' ')\n",
    "    if len(sent)>5:\n",
    "        sent_standard = [standardizeSent(standardizeSciTerms(sent))]\n",
    "        sequence = new_tokenizer.texts_to_sequences(sent_standard)\n",
    "        padded = pad_sequences(sequence, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "        y_pred1 = new_model.predict(padded)\n",
    "        y_pred = np.argmax(y_pred1, axis=1)\n",
    "        if y_pred == 1:\n",
    "            isEpi = True\n",
    "        print(sent)\n",
    "        print('Probability of epidemiology:', round(y_pred1[0][1],3))\n",
    "    \n",
    "if isEpi:\n",
    "    print(\"Abstract classification: Epidemiology\")\n",
    "else:\n",
    "    print(\"Abstract classification: Not epidemiology\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

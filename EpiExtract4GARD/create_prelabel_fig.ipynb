{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c8eef7d",
   "metadata": {},
   "source": [
    "# Create Prelabeling Figure\n",
    "The purpose of this notebook is to create a figure for the methods section\n",
    "It also contains the full pre-labeling methodology in a single notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6707066e",
   "metadata": {},
   "source": [
    "Import abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5ead0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02ec588c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PMID_getAb(PMID): \n",
    "    url = 'https://www.ebi.ac.uk/europepmc/webservices/rest/search?query=EXT_ID:'+str(PMID)+'&resulttype=core'\n",
    "    r = requests.get(url)\n",
    "    root = ET.fromstring(r.content)\n",
    "    #Titles were not called in the initial prelabeling so we cannot call them here\n",
    "    #titles = [title.text for title in root.iter('title')]\n",
    "    abstracts = [abstract.text for abstract in root.iter('abstractText')]\n",
    "    if len(abstracts) > 0 and len(abstracts[0])>5:\n",
    "        return abstracts[0]\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf451d94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The aim of this retrospective study was to determine the prevalence of lysosomal storage disorders (LSDs) in the Czech Republic. The data on cases diagnosed between 1975 and 2008 were collected and analyzed. The overall prevalence of LSDs in the Czech population (12.25 per 100,000) is comparable to that reported for the countries with well-established and advanced diagnostics of LSDs such as the Netherlands (14 per 100,000), Australia (12.9 per 100,000) and Italy (12.1 per 100,000). Relatively higher prevalence of LSDs was reported in the north of Portugal (25 per 100,000). Thirty-four different LSDs were diagnosed in a total of 478 individuals. Gaucher disease was the most frequent LSD with a birth prevalence of 1.13 per 100,000 births. The most frequent LSD groups were lipidoses, mucopolysaccharidoses, and neuronal ceroid lipofuscinoses, with combined prevalences of 5.0, 3.72, and 2.29 per 100,000 live births, respectively. Glycoproteinoses (0.57 per 100,000 live births), glycogenosis type II (0.37), and mucolipidoses (0.31) rarely occur in the Czech population, and a range of other LSDs have not been detected at all over the past three decades. Knowledge of the birth prevalence and carrier frequency of particular disorders is important in genetic counselling for calculation of the risk for the disorder in the other members of affected families. Earlier diagnosis of these disorders will permit timely intervention and may also result in lowering of the number of newborns with LSDs.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstract = PMID_getAb(20490927)\n",
    "abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2682b0fc",
   "metadata": {},
   "source": [
    "### 1. Begin prelabel v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "768edf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import tokenize\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "import string\n",
    "PUNCTUATION = set(char for char in string.punctuation)\n",
    "import csv\n",
    "import spacy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "994a9718",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html(string):\n",
    "    string = re.sub('<.{1,4}>', ' ', string)\n",
    "    string = re.sub(\"  *\", \" \" , string)\n",
    "    string = re.sub(\"^ \", \"\" , string)\n",
    "    string = re.sub(\" $\", \"\" , string)\n",
    "    string = re.sub(\"  \", \" \" , string)\n",
    "    string = string.strip()\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a533bd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SpaCy named entities are here: https://spacy.io/models/en \n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd268210",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_stats(tokens,labels):\n",
    "    i=1\n",
    "    while i<len(labels)-1:\n",
    "        if 'STAT' in labels[i]:\n",
    "            #Includes <, > number in the statistic\n",
    "            if tokens[i-1]=='<' or tokens[i-1]=='>':\n",
    "                labels[i-1]='B-STAT'\n",
    "                labels[i]='I-STAT'\n",
    "            #Includes greater than, less than, more than, etc. \n",
    "            if tokens[i-1]=='than':\n",
    "                labels[i-2]='B-STAT'\n",
    "                labels[i-1]='I-STAT'\n",
    "                labels[i]='I-STAT'\n",
    "                \n",
    "        #Combines \"This disease affects 1 in 7500 to 1 in 10,000 people\" into a single statistic phrase instead of 2\n",
    "        if 'STAT' in labels[i-1] and 'STAT' in labels[i+1] and 'STAT' not in labels[i]:\n",
    "            if tokens[i] =='to':\n",
    "                labels[i]='I-STAT'\n",
    "                labels[i+1]='I-STAT'\n",
    "            if tokens[i] =='-':\n",
    "                labels[i]='I-STAT'\n",
    "                labels[i+1]='I-STAT'\n",
    "        \n",
    "        #This gets of the type \"prevalence of 2 to 18 per 100,000\"\n",
    "        if labels[i+1]=='B-STAT':\n",
    "            if tokens[i]=='to' or tokens[i]=='-' or tokens[i-1].isdigit():\n",
    "                labels[i-1]='B-STAT'\n",
    "                labels[i]='I-STAT'\n",
    "                labels[i+1]='I-STAT'\n",
    "        i+=1\n",
    "    return tokens,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba0cea49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should take in a sentence and output each word in it with a tentative label\n",
    "def tag_NERs(sentence):\n",
    "    \n",
    "    doc = nlp(sentence)\n",
    "    tokens = [token.text for token in doc]\n",
    "    labels = ['O' for token in doc]\n",
    "    \n",
    "    i = 0\n",
    "    for token in doc:\n",
    "        if len(str(token.text).strip())==0:\n",
    "            tokens.pop(i)\n",
    "            labels.pop(i)\n",
    "            \n",
    "        else:\n",
    "            ## Epidemiologic identifier\n",
    "            if token.text.lower() in {'incidence','prevalence','prevalences','prevalence ','incidences','occurrence','occurrences'}:\n",
    "                labels[i] = 'B-EPI'\n",
    "        \n",
    "            ## Location\n",
    "            if token.ent_type_ in {'GPE','LOC'}:\n",
    "                labels[i] = str(token.ent_iob_+'-LOC')\n",
    "            if token.text in {\"worldwide\"}:\n",
    "                labels[i] = 'B-LOC'\n",
    "        \n",
    "            ## Epidemiologic Rates\n",
    "            #This gets stuff of the form 3.5/100\n",
    "            if token.text[0].isdigit() and '/' in token.text:\n",
    "                labels[i] = 'B-STAT'\n",
    "        \n",
    "            #label all percents except those preceding \"confidence interval (CI)\"\n",
    "            if token.ent_type_ in {'PERCENT'}:# and token.text not in {'95', 'CI'}:\n",
    "                if i<len(doc)-2:\n",
    "                    if doc[i+2].text in {'CI','confidence','interval','confidence interval','(CI)','(CI','CI)'}:\n",
    "                        labels[i] = 'O'\n",
    "                        labels[i+1] = 'O'\n",
    "                        labels[i+2] = 'O'\n",
    "                    elif doc[i+1].text in {'CI','confidence','interval','confidence interval','(CI)','(CI','CI)'}:\n",
    "                        labels[i] = 'O'\n",
    "                        labels[i+1] = 'O'\n",
    "                    else:\n",
    "                        labels[i] = str(token.ent_iob_+'-STAT')\n",
    "                elif i<len(doc)-1:\n",
    "                    if doc[i+1].text in {'CI','confidence','interval','confidence interval','(CI)','(CI','CI)'}:\n",
    "                        labels[i] = 'O'\n",
    "                        labels[i+1] = 'O'\n",
    "                    else:\n",
    "                        labels[i] = str(token.ent_iob_+'-STAT')        \n",
    "                else:\n",
    "                    labels[i] = str(token.ent_iob_+'-STAT')\n",
    "        \n",
    "            #These 3 get stuff of the form \"one in 35000\" or \"one in every 23043\"\n",
    "            if (token.text.lower() in {'one','1'} and i<(len(doc)-3)): \n",
    "                if doc[i+3].is_digit:\n",
    "                    labels[i] = 'B-STAT'\n",
    "                    for j in range(i+1,i+4):\n",
    "                        labels[j] = 'I-STAT'\n",
    "            if (token.text.lower() in {'one','1'} and i<(len(doc)-2)): \n",
    "                if doc[i+2].is_digit:\n",
    "                    labels[i] = 'B-STAT'\n",
    "                    labels[i+1] = 'I-STAT'\n",
    "                    labels[i+2] = 'I-STAT'\n",
    "            if (token.text.lower() in {'one','1'} and i<(len(doc)-1)):\n",
    "                if doc[i+1].is_digit:\n",
    "                    labels[i] = 'B-STAT'\n",
    "                    labels[i+1] = 'I-STAT'\n",
    "        \n",
    "            #These should get the ones of the form: 14.1 deaths per 1,000 LBs\n",
    "            #This is a big decision tree, not sure how to write it in fewer lines of code\n",
    "            #Need to get all permutations of \"a b per c d e\" where (a or b) and (c or d) is number and e is anything, but if e does not exist still need to tag a-d as STAT\n",
    "            if token.text.lower() =='per':\n",
    "                #print(i,len(doc))\n",
    "                if i>1:\n",
    "                    if i<len(doc)-3:\n",
    "                        #Resulted in better testing when not validating that words after 'per' are numbers\n",
    "                        if (doc[i-2].is_digit or doc[i-2].ent_type_ in {'CARDINAL','ORDINAL','QUANTITY','MONEY'}) or (\n",
    "                            doc[i-1].is_digit or doc[i-1].ent_type_ in {'CARDINAL','ORDINAL','QUANTITY','MONEY'}) or (\n",
    "                            doc[i+2].is_digit or doc[i+2].ent_type_ in {'CARDINAL','ORDINAL','QUANTITY','MONEY'}) or (\n",
    "                            doc[i+1].is_digit or doc[i+1].ent_type_ in {'CARDINAL','ORDINAL','QUANTITY','MONEY'}):\n",
    "                            if tokens[i-2] not in STOPWORDS and tokens[i-2] not in PUNCTUATION:\n",
    "                                labels[i-2] = 'B-STAT'\n",
    "                                #labeling also the token after the number\n",
    "                                for j in range(i-1,i+3):\n",
    "                                    labels[j]='I-STAT'\n",
    "                            else:\n",
    "                                labels[i-1] = 'B-STAT'\n",
    "                                #labeling also the token after the number\n",
    "                                for j in range(i,i+3):\n",
    "                                    labels[j]='I-STAT'\n",
    "                            \n",
    "                    if i<len(doc)-2:\n",
    "                        if (doc[i-2].is_digit or doc[i-2].ent_type_ in {'CARDINAL','ORDINAL','QUANTITY','MONEY'}) or (\n",
    "                            doc[i-1].is_digit or doc[i-1].ent_type_ in {'CARDINAL','ORDINAL','QUANTITY','MONEY'}) or (\n",
    "                            doc[i+2].is_digit or doc[i+2].ent_type_ in {'CARDINAL','ORDINAL','QUANTITY','MONEY'}) or (\n",
    "                            doc[i+1].is_digit or doc[i+1].ent_type_ in {'CARDINAL','ORDINAL','QUANTITY','MONEY'}):\n",
    "                            if tokens[i-2] not in STOPWORDS and tokens[i-2] not in PUNCTUATION:\n",
    "                                labels[i-2] = 'B-STAT'\n",
    "                                #labeling also the token after the number\n",
    "                                for j in range(i-1,i+2):\n",
    "                                    labels[j]='I-STAT'\n",
    "                            else: \n",
    "                                labels[i-1] = 'B-STAT'\n",
    "                                #labeling also the token after the number\n",
    "                                for j in range(i,i+2):\n",
    "                                    labels[j]='I-STAT'\n",
    "                    #The difference between the above and below is in labeling the token immediately after the number\n",
    "                    if i<len(doc)-1:\n",
    "                        if (doc[i-2].is_digit or doc[i-2].ent_type_ in {'CARDINAL','ORDINAL','QUANTITY','MONEY'}) or (\n",
    "                            doc[i-1].is_digit or doc[i-1].ent_type_ in {'CARDINAL','ORDINAL','QUANTITY','MONEY'}) or (\n",
    "                            doc[i+2].is_digit or doc[i+2].ent_type_ in {'CARDINAL','ORDINAL','QUANTITY','MONEY'}) or (\n",
    "                            doc[i+1].is_digit or doc[i+1].ent_type_ in {'CARDINAL','ORDINAL','QUANTITY','MONEY'}):\n",
    "                            if tokens[i-2] not in STOPWORDS and tokens[i-2] not in PUNCTUATION:\n",
    "                                labels[i-2] = 'B-STAT'\n",
    "                                #labeling also the token after if it is number\n",
    "                                for j in range(i-1,i+1):\n",
    "                                    labels[j]='I-STAT'\n",
    "                            else: \n",
    "                                labels[i-1] = 'B-STAT'\n",
    "                                #labeling also the token after the number\n",
    "                                for j in range(i,i+1):\n",
    "                                    labels[j]='I-STAT'\n",
    "                elif i>0:\n",
    "                    if i<len(doc)-3:\n",
    "                        if (doc[i-1].is_digit or doc[i-1].ent_type_ in {'CARDINAL','ORDINAL','QUANTITY','MONEY'}) or (\n",
    "                            doc[i+2].is_digit or doc[i+2].ent_type_ in {'CARDINAL','ORDINAL','QUANTITY','MONEY'}) or (\n",
    "                            doc[i+1].is_digit or doc[i+1].ent_type_ in {'CARDINAL','ORDINAL','QUANTITY','MONEY'}):\n",
    "                \n",
    "                            labels[i-1] = 'B-STAT'\n",
    "                            #labeling also the token after the number\n",
    "                            for j in range(i,i+3):\n",
    "                                labels[j]='I-STAT'\n",
    "                            \n",
    "                    if i<len(doc)-2:\n",
    "                        if (doc[i-1].is_digit or doc[i-1].ent_type_ in {'CARDINAL','ORDINAL','QUANTITY','MONEY'}) or (\n",
    "                            doc[i+2].is_digit or doc[i+2].ent_type_ in {'CARDINAL','ORDINAL','QUANTITY','MONEY'}) or (\n",
    "                            doc[i+1].is_digit or doc[i+1].ent_type_ in {'CARDINAL','ORDINAL','QUANTITY','MONEY'}):\n",
    "                        \n",
    "                            labels[i-1] = 'B-STAT'\n",
    "                            #labeling also the token after the number\n",
    "                            for j in range(i,i+2):\n",
    "                                labels[j]='I-STAT'\n",
    "                            \n",
    "                    if i<len(doc)-1:\n",
    "                        if (doc[i-1].is_digit or doc[i-1].ent_type_ in {'CARDINAL','ORDINAL','QUANTITY','MONEY'}) or (\n",
    "                            doc[i+1].is_digit or doc[i+1].ent_type_ in {'CARDINAL','ORDINAL','QUANTITY','MONEY'}):\n",
    "                    \n",
    "                            labels[i-1] = 'B-STAT'\n",
    "                            #labeling just the number if there is nothing after. \n",
    "                            for j in range(i,i+1):\n",
    "                                labels[j]='I-STAT'\n",
    "            i+=1\n",
    "\n",
    "    if len(tokens) != len(labels):\n",
    "        raise ValueError('Token/Label Length Mismatch')\n",
    "        \n",
    "    if len(tokens)>2 and len(labels)>2:\n",
    "        tokens, labels = combine_stats(tokens,labels)\n",
    "\n",
    "    return tokens, labels #This returns as type Spacy.tokens, need to convert to strings at writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca993a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_sents = tokenize.sent_tokenize(remove_html(abstract))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc41fc23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\tO\n",
      "aim\tO\n",
      "of\tO\n",
      "this\tO\n",
      "retrospective\tO\n",
      "study\tO\n",
      "was\tO\n",
      "to\tO\n",
      "determine\tO\n",
      "the\tO\n",
      "prevalence\tB-EPI\n",
      "of\tO\n",
      "lysosomal\tO\n",
      "storage\tO\n",
      "disorders\tO\n",
      "(\tO\n",
      "LSDs\tO\n",
      ")\tO\n",
      "in\tO\n",
      "the\tB-LOC\n",
      "Czech\tI-LOC\n",
      "Republic\tI-LOC\n",
      ".\tO\n",
      "\n",
      "The\tO\n",
      "data\tO\n",
      "on\tO\n",
      "cases\tO\n",
      "diagnosed\tO\n",
      "between\tO\n",
      "1975\tO\n",
      "and\tO\n",
      "2008\tO\n",
      "were\tO\n",
      "collected\tO\n",
      "and\tO\n",
      "analyzed\tO\n",
      ".\tO\n",
      "\n",
      "The\tO\n",
      "overall\tO\n",
      "prevalence\tB-EPI\n",
      "of\tO\n",
      "LSDs\tO\n",
      "in\tO\n",
      "the\tO\n",
      "Czech\tO\n",
      "population\tO\n",
      "(\tO\n",
      "12.25\tB-STAT\n",
      "per\tI-STAT\n",
      "100,000\tI-STAT\n",
      ")\tI-STAT\n",
      "is\tO\n",
      "comparable\tO\n",
      "to\tO\n",
      "that\tO\n",
      "reported\tO\n",
      "for\tO\n",
      "the\tO\n",
      "countries\tO\n",
      "with\tO\n",
      "well\tO\n",
      "-\tO\n",
      "established\tO\n",
      "and\tO\n",
      "advanced\tO\n",
      "diagnostics\tO\n",
      "of\tO\n",
      "LSDs\tO\n",
      "such\tO\n",
      "as\tO\n",
      "the\tO\n",
      "Netherlands\tB-LOC\n",
      "(\tO\n",
      "14\tB-STAT\n",
      "per\tI-STAT\n",
      "100,000\tI-STAT\n",
      ")\tI-STAT\n",
      ",\tO\n",
      "Australia\tB-LOC\n",
      "(\tO\n",
      "12.9\tB-STAT\n",
      "per\tI-STAT\n",
      "100,000\tI-STAT\n",
      ")\tI-STAT\n",
      "and\tO\n",
      "Italy\tB-LOC\n",
      "(\tO\n",
      "12.1\tB-STAT\n",
      "per\tI-STAT\n",
      "100,000\tI-STAT\n",
      ")\tI-STAT\n",
      ".\tO\n",
      "\n",
      "Relatively\tO\n",
      "higher\tO\n",
      "prevalence\tB-EPI\n",
      "of\tO\n",
      "LSDs\tO\n",
      "was\tO\n",
      "reported\tO\n",
      "in\tO\n",
      "the\tO\n",
      "north\tO\n",
      "of\tO\n",
      "Portugal\tB-LOC\n",
      "(\tO\n",
      "25\tB-STAT\n",
      "per\tI-STAT\n",
      "100,000\tI-STAT\n",
      ")\tI-STAT\n",
      ".\tO\n",
      "\n",
      "Thirty\tO\n",
      "-\tO\n",
      "four\tO\n",
      "different\tO\n",
      "LSDs\tO\n",
      "were\tO\n",
      "diagnosed\tO\n",
      "in\tO\n",
      "a\tO\n",
      "total\tO\n",
      "of\tO\n",
      "478\tO\n",
      "individuals\tO\n",
      ".\tO\n",
      "\n",
      "Gaucher\tO\n",
      "disease\tO\n",
      "was\tO\n",
      "the\tO\n",
      "most\tO\n",
      "frequent\tO\n",
      "LSD\tO\n",
      "with\tO\n",
      "a\tO\n",
      "birth\tO\n",
      "prevalence\tB-EPI\n",
      "of\tO\n",
      "1.13\tB-STAT\n",
      "per\tI-STAT\n",
      "100,000\tI-STAT\n",
      "births\tI-STAT\n",
      ".\tO\n",
      "\n",
      "The\tO\n",
      "most\tO\n",
      "frequent\tO\n",
      "LSD\tO\n",
      "groups\tO\n",
      "were\tO\n",
      "lipidoses\tO\n",
      ",\tO\n",
      "mucopolysaccharidoses\tO\n",
      ",\tO\n",
      "and\tO\n",
      "neuronal\tO\n",
      "ceroid\tO\n",
      "lipofuscinoses\tO\n",
      ",\tO\n",
      "with\tO\n",
      "combined\tO\n",
      "prevalences\tB-EPI\n",
      "of\tO\n",
      "5.0\tO\n",
      ",\tO\n",
      "3.72\tO\n",
      ",\tO\n",
      "and\tO\n",
      "2.29\tB-STAT\n",
      "per\tI-STAT\n",
      "100,000\tI-STAT\n",
      "live\tI-STAT\n",
      "births\tO\n",
      ",\tO\n",
      "respectively\tO\n",
      ".\tO\n",
      "\n",
      "Glycoproteinoses\tO\n",
      "(\tO\n",
      "0.57\tB-STAT\n",
      "per\tI-STAT\n",
      "100,000\tI-STAT\n",
      "live\tI-STAT\n",
      "births\tO\n",
      ")\tO\n",
      ",\tO\n",
      "glycogenosis\tO\n",
      "type\tO\n",
      "II\tO\n",
      "(\tO\n",
      "0.37\tO\n",
      ")\tO\n",
      ",\tO\n",
      "and\tO\n",
      "mucolipidoses\tO\n",
      "(\tO\n",
      "0.31\tO\n",
      ")\tO\n",
      "rarely\tO\n",
      "occur\tO\n",
      "in\tO\n",
      "the\tO\n",
      "Czech\tO\n",
      "population\tO\n",
      ",\tO\n",
      "and\tO\n",
      "a\tO\n",
      "range\tO\n",
      "of\tO\n",
      "other\tO\n",
      "LSDs\tO\n",
      "have\tO\n",
      "not\tO\n",
      "been\tO\n",
      "detected\tO\n",
      "at\tO\n",
      "all\tO\n",
      "over\tO\n",
      "the\tO\n",
      "past\tO\n",
      "three\tO\n",
      "decades\tO\n",
      ".\tO\n",
      "\n",
      "Knowledge\tO\n",
      "of\tO\n",
      "the\tO\n",
      "birth\tO\n",
      "prevalence\tB-EPI\n",
      "and\tO\n",
      "carrier\tO\n",
      "frequency\tO\n",
      "of\tO\n",
      "particular\tO\n",
      "disorders\tO\n",
      "is\tO\n",
      "important\tO\n",
      "in\tO\n",
      "genetic\tO\n",
      "counselling\tO\n",
      "for\tO\n",
      "calculation\tO\n",
      "of\tO\n",
      "the\tO\n",
      "risk\tO\n",
      "for\tO\n",
      "the\tO\n",
      "disorder\tO\n",
      "in\tO\n",
      "the\tO\n",
      "other\tO\n",
      "members\tO\n",
      "of\tO\n",
      "affected\tO\n",
      "families\tO\n",
      ".\tO\n",
      "\n",
      "Earlier\tO\n",
      "diagnosis\tO\n",
      "of\tO\n",
      "these\tO\n",
      "disorders\tO\n",
      "will\tO\n",
      "permit\tO\n",
      "timely\tO\n",
      "intervention\tO\n",
      "and\tO\n",
      "may\tO\n",
      "also\tO\n",
      "result\tO\n",
      "in\tO\n",
      "lowering\tO\n",
      "of\tO\n",
      "the\tO\n",
      "number\tO\n",
      "of\tO\n",
      "newborns\tO\n",
      "with\tO\n",
      "LSDs\tO\n",
      ".\tO\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('ab_figV2.tsv', \"w\") as f:\n",
    "    for sentence in abstract_sents: #For sentence in abstract\n",
    "        tokens, labels = tag_NERs(sentence)\n",
    "        for i in range(len(tokens)): #for token in sentence\n",
    "            output = str(tokens[i]) +'\\t' +str(labels[i])+'\\n'\n",
    "            f.write(output)\n",
    "            print(output[:-1])\n",
    "        f.write('\\n')\n",
    "        print()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c76175",
   "metadata": {},
   "source": [
    "### 2. Modify labels (v2 -> v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e89e543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import string\n",
    "INCLUSIVE_WORDS = {'between','around','approximately','about','<','>','roughly','relatively','over','under','than'}#less than, greater than\n",
    "EPI_MODIFIERS = {'annual','overall','estimated','weighted','nationwide','pooled','average','cumulative'}\n",
    "DATES = {'january','february','march','april','may','june','july','august','september','october','november','december'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7af6ceaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10\n"
     ]
    }
   ],
   "source": [
    "ab_tokens, ab_labels= [],[]\n",
    "with open('ab_figV2.tsv','r', encoding=\"utf-8\") as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    sentence_tokens,sentences_tags=[],[]\n",
    "    for row in reader:\n",
    "        if len(row)%2==0:\n",
    "            if len(row)==0:\n",
    "                if len(sentence_tokens) != len(sentences_tags):\n",
    "                    print('uh oh', sentence_tokens, sentences_tags, sep='\\n')\n",
    "                ab_tokens.append(sentence_tokens.copy())\n",
    "                ab_labels.append(sentences_tags.copy())\n",
    "                sentence_tokens.clear()\n",
    "                sentences_tags.clear()\n",
    "            else:\n",
    "                sentence_tokens.append(row[0])\n",
    "                sentences_tags.append(row[1])\n",
    "f.close()\n",
    "print(len(ab_tokens),len(ab_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15225c49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The',\n",
       "  'aim',\n",
       "  'of',\n",
       "  'this',\n",
       "  'retrospective',\n",
       "  'study',\n",
       "  'was',\n",
       "  'to',\n",
       "  'determine',\n",
       "  'the',\n",
       "  'prevalence',\n",
       "  'of',\n",
       "  'lysosomal',\n",
       "  'storage',\n",
       "  'disorders',\n",
       "  '(',\n",
       "  'LSDs',\n",
       "  ')',\n",
       "  'in',\n",
       "  'the',\n",
       "  'Czech',\n",
       "  'Republic',\n",
       "  '.'],\n",
       " ['The',\n",
       "  'data',\n",
       "  'on',\n",
       "  'cases',\n",
       "  'diagnosed',\n",
       "  'between',\n",
       "  '1975',\n",
       "  'and',\n",
       "  '2008',\n",
       "  'were',\n",
       "  'collected',\n",
       "  'and',\n",
       "  'analyzed',\n",
       "  '.'],\n",
       " ['The',\n",
       "  'overall',\n",
       "  'prevalence',\n",
       "  'of',\n",
       "  'LSDs',\n",
       "  'in',\n",
       "  'the',\n",
       "  'Czech',\n",
       "  'population',\n",
       "  '(',\n",
       "  '12.25',\n",
       "  'per',\n",
       "  '100,000',\n",
       "  ')',\n",
       "  'is',\n",
       "  'comparable',\n",
       "  'to',\n",
       "  'that',\n",
       "  'reported',\n",
       "  'for',\n",
       "  'the',\n",
       "  'countries',\n",
       "  'with',\n",
       "  'well',\n",
       "  '-',\n",
       "  'established',\n",
       "  'and',\n",
       "  'advanced',\n",
       "  'diagnostics',\n",
       "  'of',\n",
       "  'LSDs',\n",
       "  'such',\n",
       "  'as',\n",
       "  'the',\n",
       "  'Netherlands',\n",
       "  '(',\n",
       "  '14',\n",
       "  'per',\n",
       "  '100,000',\n",
       "  ')',\n",
       "  ',',\n",
       "  'Australia',\n",
       "  '(',\n",
       "  '12.9',\n",
       "  'per',\n",
       "  '100,000',\n",
       "  ')',\n",
       "  'and',\n",
       "  'Italy',\n",
       "  '(',\n",
       "  '12.1',\n",
       "  'per',\n",
       "  '100,000',\n",
       "  ')',\n",
       "  '.'],\n",
       " ['Relatively',\n",
       "  'higher',\n",
       "  'prevalence',\n",
       "  'of',\n",
       "  'LSDs',\n",
       "  'was',\n",
       "  'reported',\n",
       "  'in',\n",
       "  'the',\n",
       "  'north',\n",
       "  'of',\n",
       "  'Portugal',\n",
       "  '(',\n",
       "  '25',\n",
       "  'per',\n",
       "  '100,000',\n",
       "  ')',\n",
       "  '.'],\n",
       " ['Thirty',\n",
       "  '-',\n",
       "  'four',\n",
       "  'different',\n",
       "  'LSDs',\n",
       "  'were',\n",
       "  'diagnosed',\n",
       "  'in',\n",
       "  'a',\n",
       "  'total',\n",
       "  'of',\n",
       "  '478',\n",
       "  'individuals',\n",
       "  '.'],\n",
       " ['Gaucher',\n",
       "  'disease',\n",
       "  'was',\n",
       "  'the',\n",
       "  'most',\n",
       "  'frequent',\n",
       "  'LSD',\n",
       "  'with',\n",
       "  'a',\n",
       "  'birth',\n",
       "  'prevalence',\n",
       "  'of',\n",
       "  '1.13',\n",
       "  'per',\n",
       "  '100,000',\n",
       "  'births',\n",
       "  '.'],\n",
       " ['The',\n",
       "  'most',\n",
       "  'frequent',\n",
       "  'LSD',\n",
       "  'groups',\n",
       "  'were',\n",
       "  'lipidoses',\n",
       "  ',',\n",
       "  'mucopolysaccharidoses',\n",
       "  ',',\n",
       "  'and',\n",
       "  'neuronal',\n",
       "  'ceroid',\n",
       "  'lipofuscinoses',\n",
       "  ',',\n",
       "  'with',\n",
       "  'combined',\n",
       "  'prevalences',\n",
       "  'of',\n",
       "  '5.0',\n",
       "  ',',\n",
       "  '3.72',\n",
       "  ',',\n",
       "  'and',\n",
       "  '2.29',\n",
       "  'per',\n",
       "  '100,000',\n",
       "  'live',\n",
       "  'births',\n",
       "  ',',\n",
       "  'respectively',\n",
       "  '.'],\n",
       " ['Glycoproteinoses',\n",
       "  '(',\n",
       "  '0.57',\n",
       "  'per',\n",
       "  '100,000',\n",
       "  'live',\n",
       "  'births',\n",
       "  ')',\n",
       "  ',',\n",
       "  'glycogenosis',\n",
       "  'type',\n",
       "  'II',\n",
       "  '(',\n",
       "  '0.37',\n",
       "  ')',\n",
       "  ',',\n",
       "  'and',\n",
       "  'mucolipidoses',\n",
       "  '(',\n",
       "  '0.31',\n",
       "  ')',\n",
       "  'rarely',\n",
       "  'occur',\n",
       "  'in',\n",
       "  'the',\n",
       "  'Czech',\n",
       "  'population',\n",
       "  ',',\n",
       "  'and',\n",
       "  'a',\n",
       "  'range',\n",
       "  'of',\n",
       "  'other',\n",
       "  'LSDs',\n",
       "  'have',\n",
       "  'not',\n",
       "  'been',\n",
       "  'detected',\n",
       "  'at',\n",
       "  'all',\n",
       "  'over',\n",
       "  'the',\n",
       "  'past',\n",
       "  'three',\n",
       "  'decades',\n",
       "  '.'],\n",
       " ['Knowledge',\n",
       "  'of',\n",
       "  'the',\n",
       "  'birth',\n",
       "  'prevalence',\n",
       "  'and',\n",
       "  'carrier',\n",
       "  'frequency',\n",
       "  'of',\n",
       "  'particular',\n",
       "  'disorders',\n",
       "  'is',\n",
       "  'important',\n",
       "  'in',\n",
       "  'genetic',\n",
       "  'counselling',\n",
       "  'for',\n",
       "  'calculation',\n",
       "  'of',\n",
       "  'the',\n",
       "  'risk',\n",
       "  'for',\n",
       "  'the',\n",
       "  'disorder',\n",
       "  'in',\n",
       "  'the',\n",
       "  'other',\n",
       "  'members',\n",
       "  'of',\n",
       "  'affected',\n",
       "  'families',\n",
       "  '.'],\n",
       " ['Earlier',\n",
       "  'diagnosis',\n",
       "  'of',\n",
       "  'these',\n",
       "  'disorders',\n",
       "  'will',\n",
       "  'permit',\n",
       "  'timely',\n",
       "  'intervention',\n",
       "  'and',\n",
       "  'may',\n",
       "  'also',\n",
       "  'result',\n",
       "  'in',\n",
       "  'lowering',\n",
       "  'of',\n",
       "  'the',\n",
       "  'number',\n",
       "  'of',\n",
       "  'newborns',\n",
       "  'with',\n",
       "  'LSDs',\n",
       "  '.']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ab_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab13fc96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-EPI',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-LOC',\n",
       "  'I-LOC',\n",
       "  'I-LOC',\n",
       "  'O'],\n",
       " ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
       " ['O',\n",
       "  'O',\n",
       "  'B-EPI',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-STAT',\n",
       "  'I-STAT',\n",
       "  'I-STAT',\n",
       "  'I-STAT',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-LOC',\n",
       "  'O',\n",
       "  'B-STAT',\n",
       "  'I-STAT',\n",
       "  'I-STAT',\n",
       "  'I-STAT',\n",
       "  'O',\n",
       "  'B-LOC',\n",
       "  'O',\n",
       "  'B-STAT',\n",
       "  'I-STAT',\n",
       "  'I-STAT',\n",
       "  'I-STAT',\n",
       "  'O',\n",
       "  'B-LOC',\n",
       "  'O',\n",
       "  'B-STAT',\n",
       "  'I-STAT',\n",
       "  'I-STAT',\n",
       "  'I-STAT',\n",
       "  'O'],\n",
       " ['O',\n",
       "  'O',\n",
       "  'B-EPI',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-LOC',\n",
       "  'O',\n",
       "  'B-STAT',\n",
       "  'I-STAT',\n",
       "  'I-STAT',\n",
       "  'I-STAT',\n",
       "  'O'],\n",
       " ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
       " ['O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-EPI',\n",
       "  'O',\n",
       "  'B-STAT',\n",
       "  'I-STAT',\n",
       "  'I-STAT',\n",
       "  'I-STAT',\n",
       "  'O'],\n",
       " ['O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-EPI',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-STAT',\n",
       "  'I-STAT',\n",
       "  'I-STAT',\n",
       "  'I-STAT',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O'],\n",
       " ['O',\n",
       "  'O',\n",
       "  'B-STAT',\n",
       "  'I-STAT',\n",
       "  'I-STAT',\n",
       "  'I-STAT',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O'],\n",
       " ['O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-EPI',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O'],\n",
       " ['O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ab_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e44c314c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_stats(tokens,labels): #this is with lists of tokens, not lists of sentences which are lists of tokens\n",
    "    for i in range(1,len(labels)-1):\n",
    "        if 'STAT' in labels[i]:\n",
    "            #Includes <, > number in the statistic\n",
    "            if tokens[i-1]=='<' or tokens[i-1]=='>':\n",
    "                labels[i-1]='B-STAT'\n",
    "                labels[i]='I-STAT'\n",
    "            #Includes greater than, less than, more than, etc. \n",
    "            if tokens[i-1]=='than':\n",
    "                labels[i-2]='B-STAT'\n",
    "                labels[i-1]='I-STAT'\n",
    "                labels[i]='I-STAT'\n",
    "                \n",
    "        #Combines \"This disease affects 1 in 7500 to 1 in 10,000 people\" into a single statistic phrase instead of 2\n",
    "        if 'STAT' in labels[i-1] and 'STAT' in labels[i+1] and 'STAT' not in labels[i]:\n",
    "            if tokens[i] =='to':\n",
    "                labels[i]='I-STAT'\n",
    "                labels[i+1]='I-STAT'\n",
    "            if tokens[i] =='-':\n",
    "                labels[i]='I-STAT'\n",
    "                labels[i+1]='I-STAT'\n",
    "        \n",
    "        #This gets of the type \"prevalence of 2 to 18 per 100,000\"\n",
    "        if labels[i+1]=='B-STAT':\n",
    "            if tokens[i]=='to' or tokens[i]=='-' or tokens[i-1].isdigit():\n",
    "                labels[i-1]='B-STAT'\n",
    "                labels[i]='I-STAT'\n",
    "                labels[i+1]='I-STAT'\n",
    "    return tokens,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9cb75e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_labels(tokens,labels):\n",
    "    if len(tokens)!=len(labels):\n",
    "        raise IndexError(\"Num Sentences {} and Num Sentence Labels {} Mismatch\".format(len(tokens),len(labels)))\n",
    "    for i in range(len(tokens)):\n",
    "        if len(tokens[i])!=len(labels[i]):\n",
    "            raise IndexError(\"Sentence Length {} and Label Length {} Mismatch\".format(len(tokens[i]),len(labels[i])))\n",
    "        '''\n",
    "        #Comparison loop\n",
    "        for j in range(2,len(tokens[i])-2):\n",
    "            if labels[i][j]=='B-STAT':\n",
    "                print('BEFORE')\n",
    "                print(tokens[i][j-2:j+3])\n",
    "                print(labels[i][j-2:j+3])\n",
    "                print('')\n",
    "        '''\n",
    "        for j in range(len(tokens[i])):\n",
    "            if tokens[i][j]=='prevalent' or tokens[i][j]=='occurs':\n",
    "                labels[i][j] = 'B-EPI'\n",
    "        \n",
    "        for j in range(2,len(tokens[i])-2):\n",
    "            if tokens[i][j].lower() in DATES:\n",
    "                labels[i][j]='O'\n",
    "                labels[i][j+1]='O'\n",
    "                labels[i][j+2]='O'\n",
    "                \n",
    "        for j in range(2,len(tokens[i])-2):\n",
    "            if tokens[i][j-1].lower() in DATES:\n",
    "                labels[i][j-1]='O'\n",
    "                labels[i][j]='O'\n",
    "                labels[i][j+1]='O'\n",
    "                labels[i][j+2]='O'\n",
    "            \n",
    "            #Ensures that there is not already a label\n",
    "            if labels[i][j] in {'O','B-STAT','I-STAT'}:\n",
    "                #relabel all of the percentages\n",
    "                '''\n",
    "                if '%' in tokens[i][j]:\n",
    "                    print('BEFORE')\n",
    "                    print(tokens[i][j-2:j+3])\n",
    "                    print(labels[i][j-2:j+3])\n",
    "                    print('')\n",
    "                '''\n",
    "                #only include small percentages of the form '0.*'\n",
    "                if (('%' in tokens[i][j] or ('per' in tokens[i][j].lower() and 'cent' in tokens[i][j+1].lower()) or 'percent' in tokens[i][j].lower()\n",
    "                    ) and not (re.match(r\"^0\\.\", tokens[i][j]\n",
    "                    ) or re.match(r\"^0\\.\", tokens[i][j-1]\n",
    "                    ) or re.match(r\"^0\\.\", tokens[i][j-2]))\n",
    "                    ) and not ('B-EPI' in set(labels[i][j-2:j+3]) or 'I-EPI' in set(labels[i][j-2:j+3])):\n",
    "                    labels[i][j-2] = 'O'\n",
    "                    labels[i][j-1] = 'O'\n",
    "                    labels[i][j] = 'O'\n",
    "                    labels[i][j+1] = 'O'\n",
    "                    labels[i][j+2] = 'O'\n",
    "                '''\n",
    "                if '%' in tokens[i][j]:\n",
    "                    print('AFTER')\n",
    "                    print(tokens[i][j-2:j+3])\n",
    "                    print(labels[i][j-2:j+3])\n",
    "                    print('\\n')\n",
    "                '''\n",
    "                #gets of the form 1:100,000\n",
    "                #tag word before except if it is 'of' or label before is there\n",
    "                if tokens[i][j][0].isdigit() and ':' in tokens[i][j]:\n",
    "                    #print(tokens[i][j-2:j+3])\n",
    "                    #print(labels[i][j-2:j+3])\n",
    "                    #print(tokens[i][j].split(':'))\n",
    "                    #Exclude\n",
    "                    if ((len(tokens[i][j])==5 and not tokens[i][j+1][0].isdigit()) or len(tokens[i][j].split(':')[0])>3):\n",
    "                        pass\n",
    "                        #continue\n",
    "                    #Exclude\n",
    "                    elif 'ratio' in tokens[i][j-2:j-1]:\n",
    "                        pass\n",
    "                        #continue\n",
    "                    else:\n",
    "                        if tokens[i][j-1].lower() in INCLUSIVE_WORDS:\n",
    "                            labels[i][j-1]='B-STAT'\n",
    "                            labels[i][j]='I-STAT'\n",
    "                        elif tokens[i][j-2].lower() in INCLUSIVE_WORDS:\n",
    "                            labels[i][j-2]='B-STAT'\n",
    "                            labels[i][j-1]='I-STAT'\n",
    "                            labels[i][j]='I-STAT'\n",
    "                        else:\n",
    "                            labels[i][j]='B-STAT'\n",
    "\n",
    "                        if tokens[i][j+1][0].isdigit():\n",
    "                            labels[i][j+1]='I-STAT'\n",
    "                            if (tokens[i][j+2].lower() not in STOPWORDS and tokens[i][j+2] not in PUNCTUATION) and labels[i][j+2] in {'O','B-STAT','I-STAT'}:\n",
    "                                labels[i][j+2]='I-STAT'\n",
    "                                #Could potentially cause an indexing issue?\n",
    "                                labels[i][j+3]='I-STAT'\n",
    "                        if (tokens[i][j+1].lower() not in STOPWORDS and tokens[i][j+1] not in PUNCTUATION) and labels[i][j+1] in {'O','B-STAT','I-STAT'}:\n",
    "                            labels[i][j+1]='I-STAT'\n",
    "                            #Checks to make sure it is not already tagged\n",
    "                            if labels[i][j+2] in {'O','B-STAT'}:\n",
    "                                labels[i][j+2]='I-STAT'\n",
    "                                \n",
    "                            \n",
    "        for j in range(1,len(tokens[i])-1):\n",
    "            if tokens[i][j].lower() == 'unknown' and (labels[i][j+1]=='B-EPI' or labels[i][j+1]=='I-EPI'):\n",
    "                labels[i][j]='B-STAT'\n",
    "            if tokens[i][j].lower() == 'global' and (labels[i][j+1]=='B-EPI' or labels[i][j+1]=='I-EPI'):\n",
    "                labels[i][j]='B-LOC'\n",
    "            #Gets the ones who have cut off numbers\n",
    "            if labels[i][j]=='I-STAT' and labels[i][j+1]=='O':\n",
    "                if tokens[i][j+1][0].isdigit():\n",
    "                    labels[i][j+1]='I-STAT'\n",
    "            \n",
    "            if labels[i][j]=='B-STAT':\n",
    "                #This is supposed to match years, but not sure how well, did not test, copied from stackoverflow\n",
    "                if re.match(r\"^[12][0-9]{3}$\",tokens[i][j].split('/')[0]):\n",
    "                    labels[i][j]='O'\n",
    "                #gets rid of incessant 'was' being tagged\n",
    "                if tokens[i][j].lower() in STOPWORDS:\n",
    "                    labels[i][j]='O'\n",
    "                    labels[i][j+1]='B-STAT'\n",
    "            #Lengthens tags to include descriptors, there could bee more to include, but did not pop out during testing\n",
    "            if (labels[i][j-1] =='I-STAT' or labels[i][j-1] =='B-STAT') and labels[i][j] =='O':\n",
    "                if tokens[i][j+1] in {'births','LBs','LB','birth'}:\n",
    "                    labels[i][j]='I-STAT'\n",
    "                    labels[i][j+1]='I-STAT'\n",
    "                elif tokens[i][j] in {'births','LBs','LB','birth'}:\n",
    "                    labels[i][j]='I-STAT'\n",
    "            #This should also lengthen epi tags a little bit to include descriptors\n",
    "            if labels[i][j]=='B-EPI' and tokens[i][j-1].lower() in EPI_MODIFIERS:\n",
    "                labels[i][j-1]='B-EPI'\n",
    "                labels[i][j]='I-EPI'\n",
    "            \n",
    "            ## This was not in V3.1, this is the final change that created the V3.2 dataset. Everything else is the same\n",
    "            #This should remove isolated stats that do not have epi in the rest of the sentence\n",
    "            if labels[i][j-1]=='B-STAT' and labels[i][j]!='I-STAT' and 'B-EPI' not in labels[i]:\n",
    "                if '/' in tokens[i][j-1]:\n",
    "                    if len(tokens[i][j-1])<9 or '+' in tokens[i][j-1] or '-' in tokens[i][j-1] or '±' in tokens[i][j-1]:\n",
    "                        labels[i][j-1]=='O'\n",
    "                    else:\n",
    "                        #leave out the ones like ['1.21/10,000', 'individuals', ')'] ['B-STAT', 'O', 'O']\n",
    "                        pass\n",
    "                else:\n",
    "                    labels[i][j-1]=='O'\n",
    "\n",
    "        tokens, labels = combine_stats(tokens,labels)\n",
    "            \n",
    "    return tokens, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "912f0941",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_ab_tokens, mod_ab_labels = modify_labels(ab_tokens,ab_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5ce9e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ab_figV3.tsv', \"w\") as f:\n",
    "    for i in range(len(mod_ab_tokens)): #For sentence in list of sentences\n",
    "        for j in range(len(mod_ab_tokens[i])): #for token in sentence\n",
    "            output = str(mod_ab_tokens[i][j]) +'\\t' +str(mod_ab_labels[i][j])+'\\n'\n",
    "            f.write(output)\n",
    "        f.write('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e180ae3b",
   "metadata": {},
   "source": [
    "### 3. Modify labels (v3 -> v4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bebff7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a6121fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## STOPWORDS\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "import string\n",
    "PUNCTUATION = set(char for char in string.punctuation)\n",
    "STOPWORDS = STOPWORDS.union(PUNCTUATION)\n",
    "\n",
    "## EPI PHRASES\n",
    "EPI_ROOTS = {'incidence','prevalence','occurrence','incidences','prevalences','occurrences'}\n",
    "EPI_CONDITIONAL_ROOTS = {'affect','occurs','affects','frequency','frequencies'} #exluding affect\n",
    "EPI_PREMODIFIERS = {'annual','overall','estimated','weighted','nationwide','pooled','average','cumulative','annualized','age-adjusted','sex-adjusted','associated','population-based','calculated','combined','corrected','familial','race/ethnicity-specific','race-specific','birth','community-based','point','total','age-specific','ethnicity-specific'}\n",
    "EPI_POSTMODIFIERS = {'estimate','estimates','rate','rates'}\n",
    "\n",
    "EPI_PHRASES = set()\n",
    "for root in EPI_ROOTS:\n",
    "    for premod in EPI_PREMODIFIERS:\n",
    "        for postmod in EPI_POSTMODIFIERS:\n",
    "            EPI_PHRASES.add(' '.join([premod,root,postmod]))\n",
    "            EPI_PHRASES.add(' '.join([premod,root]))\n",
    "            EPI_PHRASES.add(' '.join([root,postmod]))\n",
    "EPI_PHRASES = EPI_PHRASES.union(EPI_ROOTS)\n",
    "            \n",
    "EPI_CONDITIONAL_PHRASES = set()\n",
    "for root in EPI_CONDITIONAL_ROOTS:\n",
    "    for premod in EPI_PREMODIFIERS:\n",
    "        for postmod in EPI_POSTMODIFIERS:\n",
    "            EPI_CONDITIONAL_PHRASES.add(' '.join([premod,root,postmod]))\n",
    "            EPI_CONDITIONAL_PHRASES.add(' '.join([premod,root]))\n",
    "            EPI_CONDITIONAL_PHRASES.add(' '.join([root,postmod]))\n",
    "EPI_CONDITIONAL_PHRASES = EPI_CONDITIONAL_PHRASES.union(EPI_CONDITIONAL_ROOTS)\n",
    "            \n",
    "## DATES\n",
    "#ADDING every possible English date presentation to a set. easier that using spaCy and messing up the tokenization\n",
    "MONTHS = {'january','february','march','april','may','june','july','august','september','october','november','december'}\n",
    "YEARS = {str(i) for i in range(1900, 2022)}\n",
    "DAYS= {str(i) for i in range(1, 32)}\n",
    "DATES = set()\n",
    "for year in YEARS:\n",
    "    for month in MONTHS:\n",
    "        for day in DAYS:\n",
    "            DATES.add(' '.join([day,month,year]))\n",
    "            DATES.add(' '.join([month,day,year]))\n",
    "            DATES.add(' '.join([month,day,',',year]))\n",
    "            DATES.add(' '.join([month,day+',',year]))\n",
    "            DATES.add(' '.join([month,year]))\n",
    "DATES = DATES.union(YEARS)\n",
    "\n",
    "## ETHNITICIES\n",
    "#from a Wikipedia Webscrape\n",
    "import json\n",
    "with open('../Final Dataset Prep/ethnicities.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    ETHNICITIES = {ethnicity for ethnicity in data['ethnicities']}      \n",
    "    \n",
    "## DISEASES\n",
    "from extract_abs import load_GARD_diseases\n",
    "GARD_dict, max_length = load_GARD_diseases()\n",
    "\n",
    "## BIOLOGICAL SEX\n",
    "SEX = {'male','female','males','females','girl','girls','boy','boys','man','men','woman','women','intersex','XYY','XXXY','XXXXY','klinefelter syndrome','klinefelter'}\n",
    "#Klinefelter included, not including Turner syndrome because it is a rare disease\n",
    "\n",
    "## RANGE WORDS (those are different types of dashes)\n",
    "RANGE_WORDS = {'-', 'and', 'to', 'until', '‑', '‒', '–', '—', '―'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cc18885f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10\n"
     ]
    }
   ],
   "source": [
    "ab_tokens, ab_labels = [],[]\n",
    "with open('ab_figV3.tsv','r', encoding=\"utf-8\") as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    sentence_tokens,sentences_tags=[],[]\n",
    "    for row in reader:\n",
    "        if len(row)%2==0:\n",
    "            if len(row)==0:\n",
    "                if len(sentence_tokens) != len(sentences_tags):\n",
    "                    print('uh oh', sentence_tokens, sentences_tags, sep='\\n')\n",
    "                ab_tokens.append(sentence_tokens.copy())\n",
    "                ab_labels.append(sentences_tags.copy())\n",
    "                sentence_tokens.clear()\n",
    "                sentences_tags.clear()\n",
    "            else:\n",
    "                sentence_tokens.append(row[0])\n",
    "                sentences_tags.append(row[1])\n",
    "f.close()\n",
    "print(len(ab_tokens),len(ab_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "70a72036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_dates(sentence_tokens,sentence_labels): #this is with lists of sentence_tokens, not lists of sentences which are lists of sentence_tokens\n",
    "    for i in range(1,len(sentence_labels)-1):\n",
    "        #Combines \n",
    "        if 'DATE' in sentence_labels[i-1] and 'DATE' in sentence_labels[i+1] and 'DATE' not in sentence_labels[i]:\n",
    "            if sentence_tokens[i] in RANGE_WORDS:\n",
    "                sentence_labels[i]='I-DATE'\n",
    "                sentence_labels[i+1]='I-DATE'\n",
    "        \n",
    "    return sentence_tokens,sentence_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "927373e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diseases = GARD_dict.keys()\n",
    "# Epi modifiers (not all adjectives) = EPI_PHRASES, EPI_CONDITIONAL_PHRASES\n",
    "# Biological sex (not gender) = SEX\n",
    "# Dates and date ranges (for the study period) = DATES\n",
    "# Ethnicity (also includes races and nationality due to greedy categorical search) = ETHNICITIES\n",
    "\n",
    "def tag_everything(sentence_tokens,sentence_labels):   \n",
    "    i=0\n",
    "    \n",
    "    SentHasSTAT = bool('B-STAT' in sentence_labels)\n",
    "    \n",
    "    while i <len(sentence_tokens):       \n",
    "        if (len(sentence_tokens)-i) < max_length:\n",
    "            compare_length=len(sentence_tokens)-i\n",
    "        else:\n",
    "            compare_length = max_length\n",
    "        #Compares longest sequences first and goes down until there is a match\n",
    "        exit = False\n",
    "        while compare_length>0:\n",
    "            s = ' '.join(sentence_tokens[i:i+compare_length])\n",
    "            \n",
    "            #DISEASES\n",
    "            if s.lower() in GARD_dict.keys():\n",
    "                sentence_labels[i] = 'B-DIS'\n",
    "                for j in range(i+1,i+compare_length):\n",
    "                    sentence_labels[j] = 'I-DIS'\n",
    "                i+=compare_length-1\n",
    "                break\n",
    "            \n",
    "            #EPI TYPE PHRASES\n",
    "            elif s.lower() in EPI_PHRASES or (s.lower() in EPI_CONDITIONAL_PHRASES and SentHasSTAT):\n",
    "                #print('EPI TYPE: ',s)\n",
    "                sentence_labels[i] = 'B-EPI'\n",
    "                for j in range(i+1,i+compare_length):\n",
    "                    sentence_labels[j] = 'I-EPI'\n",
    "                i+=compare_length-1\n",
    "                break\n",
    "            \n",
    "            #SEX\n",
    "            elif s.lower() in SEX:\n",
    "                #print('SEX: ',s)\n",
    "                sentence_labels[i] = 'B-SEX'\n",
    "                for j in range(i+1,i+compare_length):\n",
    "                    sentence_labels[j] = 'I-SEX'\n",
    "                i+=compare_length-1\n",
    "                break\n",
    "            \n",
    "            #DATES\n",
    "            elif s.lower() in DATES:\n",
    "                #print('DATE: ',s)\n",
    "                sentence_labels[i] = 'B-DATE'\n",
    "                for j in range(i+1,i+compare_length):\n",
    "                    sentence_labels[j] = 'I-DATE'\n",
    "                i+=compare_length-1\n",
    "                break\n",
    "            \n",
    "            #ETHNICITIES\n",
    "            elif s in ETHNICITIES:\n",
    "                #print('RAW ETHNICITY: ',s)\n",
    "                sentence_labels[i] = 'B-ETHN'\n",
    "                for j in range(i+1,i+compare_length):\n",
    "                    sentence_labels[j] = 'I-ETHN'\n",
    "                i+=compare_length-1\n",
    "                break\n",
    "            \n",
    "            #stem every word in the phrase and see if it matches\n",
    "            elif ' '.join([porter.stem(word) for word in s.split()]) in ETHNICITIES:\n",
    "                #print('STEMMED ETHNICITY: ',s)\n",
    "                sentence_labels[i] = 'B-ETHN'\n",
    "                for j in range(i+1,i+compare_length):\n",
    "                    sentence_labels[j] = 'I-ETHN'\n",
    "                i+=compare_length-1\n",
    "                break\n",
    "                \n",
    "            else:\n",
    "                compare_length-=1\n",
    "        i+=1  \n",
    "    return sentence_tokens, sentence_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2df14cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_labels(tokens,labels):\n",
    "    if len(tokens)!=len(labels):\n",
    "        raise IndexError(\"Num Sentences {} and Num Sentence Labels {} Mismatch\".format(len(tokens),len(labels)))\n",
    "\n",
    "    for i in range(len(tokens)):\n",
    "        if len(tokens[i])!=len(labels[i]):\n",
    "            raise IndexError(\"Sentence Length {} and Label Length {} Mismatch\".format(len(tokens[i]),len(labels[i])))\n",
    "            \n",
    "        tokens[i], labels[i] = tag_everything(tokens[i], labels[i])\n",
    "        tokens[i], labels[i] = combine_dates(tokens[i], labels[i])\n",
    "        \n",
    "    return tokens, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e6e86b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_ab_tokens, mod_ab_labels = modify_labels(ab_tokens,ab_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a9cee475",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ab_figV4.tsv', \"w\") as f:\n",
    "    for i in range(len(mod_ab_tokens)): #For sentence in list of sentences\n",
    "        for j in range(len(mod_ab_tokens[i])): #for token in sentence\n",
    "            output = str(mod_ab_tokens[i][j]) +'\\t' +str(mod_ab_labels[i][j])+'\\n'\n",
    "            f.write(output)\n",
    "        f.write('\\n')\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

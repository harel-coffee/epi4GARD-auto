{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "042a7bd2",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to test (and archive) functions that will be put into *extract_abs.py*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19935a84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "#!{sys.executable} -m pip install nltk\n",
    "#!{sys.executable} -m pip install Unidecode\n",
    "#!{sys.executable} -m pip install spacy\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import tokenize\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "import string\n",
    "PUNCTUATION = set(char for char in string.punctuation)\n",
    "import csv\n",
    "import spacy\n",
    "import re\n",
    "from transformers import BertConfig, AutoModelForTokenClassification, BertTokenizer, pipeline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import classify_abs\n",
    "#import extract_abs\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad6a113",
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract_abs import autosearch, str2sents, get_diseases, load_GARD_diseases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d540b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Section: Prepare ML/DL Models\n",
    "# This fuction prepares the model. Should call before running in notebook.\n",
    "def init_NER_pipeline(name_or_path_to_model_folder = \"ncats/EpiExtract4GARD-v2\"): #NER_pipeline, labels = init_NER_pipeline()\n",
    "    tokenizer = BertTokenizer.from_pretrained(name_or_path_to_model_folder)\n",
    "    custommodel = AutoModelForTokenClassification.from_pretrained(name_or_path_to_model_folder)\n",
    "    customNER = pipeline('ner', custommodel, tokenizer=tokenizer, aggregation_strategy='simple')\n",
    "    \n",
    "    config = BertConfig.from_pretrained(name_or_path_to_model_folder)\n",
    "    labels = {re.sub(\".-\",\"\",label) for label in config.label2id.keys() if label != \"O\"}\n",
    "    return customNER, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e38e4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input: Sentences & Model Outputs Output: Dictionary with all entity types (dynamic to fit multiple models)\n",
    "#model_outputs is list of NER_pipeline outputs\n",
    "#labels are a set of all the possible entities (not including \"O\"). This is a misnomer. Was originally named \"entities\" but changed to not get confused with other code\n",
    "def parse_info(sentences, model_outputs, labels, GARD_dict, max_length):\n",
    "    #do not use dict.fromkeys(labels,set()) as the value is a single instance which all keys point to. The value is therefore effectively immutable. See: https://docs.python.org/3/library/stdtypes.html?highlight=dict%20fromkeys#dict.fromkeys\n",
    "    output_dict = {label:([] if label =='STAT' else set()) for label in labels}\n",
    "    for output in model_outputs:\n",
    "        #This abstracts the labels so that models with different types and numbers of labels can be used.\n",
    "        for label in labels:\n",
    "            if label == 'STAT':\n",
    "                #no unique filtering for stats, also means that results stay in order\n",
    "                output_dict[label]+=[entity_dict['word'] for entity_dict in output if entity_dict['entity_group'] ==label]\n",
    "            else:\n",
    "                #used sets to auto-filter duplicates\n",
    "                output_dict[label].update({entity_dict['word'] for entity_dict in output if entity_dict['entity_group'] ==label})\n",
    "                \n",
    "    if 'DIS' not in labels:\n",
    "        for sentence in sentences:\n",
    "            diseases,ids = get_diseases(sentence, GARD_dict, max_length)\n",
    "            output_dict['DIS'] = diseases\n",
    "            output_dict['IDS'] = ids\n",
    "\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0af702",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracts Disease GARD ID, Disease Name, Location, Epidemiologic Identifier, Epidemiologic Statistic given a PubMed ID\n",
    "def PMID_extraction(pmid, NER_pipeline, labels, GARD_dict, max_length): #extraction = PMID_extraction(pmid, NER_pipeline, labels, GARD_dict, max_length)\n",
    "    text = classify_abs.PMID_getAb(pmid)\n",
    "    if len(text)>5:\n",
    "        sentences = str2sents(text)\n",
    "        model_outputs = [NER_pipeline(sent) for sent in sentences]\n",
    "        output_dict = parse_info(sentences, model_outputs, labels, GARD_dict, max_length)\n",
    "        output_dict['ABSTRACT'] = text\n",
    "        return output_dict\n",
    "    else:\n",
    "        out = ['ABSTRACT']\n",
    "        out+=list(labels)\n",
    "        output_dict =dict.fromkeys(out,\"N/A\")\n",
    "        output_dict['ABSTRACT'] = '*ABSTRACT NOT FOUND*'\n",
    "        return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b091101",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract if you already have the text and you do not want epi_predictions (this makes things much faster)\n",
    "#extraction = abstract_extraction(text, NER_pipeline, labels, GARD_dict, max_length)\n",
    "def abstract_extraction(text, NER_pipeline, labels, GARD_dict, max_length): \n",
    "    if len(text)>5:\n",
    "        sentences = str2sents(text)\n",
    "        model_outputs = [NER_pipeline(sent) for sent in sentences]\n",
    "        output_dict = parse_info(sentences, model_outputs, labels, GARD_dict, max_length)\n",
    "        output_dict['ABSTRACT'] = text\n",
    "        return output_dict\n",
    "    else:\n",
    "        out = ['ABSTRACT']\n",
    "        out+=list(labels)\n",
    "        output_dict =dict.fromkeys(out,\"N/A\")\n",
    "        output_dict['ABSTRACT'] = '*ABSTRACT NOT FOUND*'\n",
    "        return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac496bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "disease,ids = set(), set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1bf6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed1a5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce77cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "disease.add('fire')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c129d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71168b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a6b75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This ensures that there is a standardized ordering of df columns while ensuring dynamics with multiple models. This is used by search_term_extraction.\n",
    "def order_labels(entities):\n",
    "    ordered_labels = []\n",
    "    label_order = ['ABRV','EPI','STAT','LOC','DATE','SEX','ETHN']\n",
    "    ordered_labels = [label for label in label_order if label in entities]\n",
    "    #This adds any extra entities (from yet-to-be-created models) to the end of the ordered list of labels \n",
    "    for entity in entities:\n",
    "        if entity not in label_order:\n",
    "            ordered_labels.append(entity)\n",
    "    return ordered_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16940a39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Given a search term and max results to return, this will acquire PubMed IDs and Title+Abstracts and Classify them as epidemiological.\n",
    "#It then extracts Epidemiologic Information[Disease GARD ID, Disease Name, Location, Epidemiologic Identifier, Epidemiologic Statistic] for each abstract\n",
    "# results = search_term_extraction(search_term, maxResults, NER_pipeline, labels, GARD_dict, max_length, nlp, nlpSci, nlpSci2, classify_model, classify_tokenizer)\n",
    "def search_term_extraction(search_term, maxResults, #for abstract search\n",
    "                           NER_pipeline, labels, GARD_dict, max_length, #for extraction \n",
    "                           nlp, nlpSci, nlpSci2, classify_model, classify_tokenizer): #for classification\n",
    "    #Format of Output\n",
    "    ordered_labels = order_labels(labels)\n",
    "    columns = ['PMID', 'ABRSTRACT','EPI_PROB','IsEpi','IDS','DIS']+ordered_labels\n",
    "    results = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    ##Check to see if search term maps to anything in the GARD dictionary, if so it pulls up all synonyms for the search\n",
    "    search_term_list = autosearch(search_term, GARD_dict)\n",
    "    \n",
    "    #Gather title+abstracts into a dictionary {pmid:abstract}\n",
    "    pmid_abs = classify_abs.search_getAbs(search_term_list,maxResults)\n",
    "    \n",
    "    for pmid, abstract in pmid_abs.items():\n",
    "        epi_prob, isEpi = classify_abs.getTextPredictions(abstract, nlp, nlpSci, nlpSci2, classify_model, classify_tokenizer)\n",
    "        if isEpi:\n",
    "            #Preprocessing Functions for Extraction\n",
    "            sentences = str2sents(abstract)\n",
    "            model_outputs = [NER_pipeline(sent) for sent in sentences]\n",
    "            extraction = parse_info(sentences, model_outputs, labels, GARD_dict, max_length)\n",
    "            extraction.update({'PMID':pmid, 'ABRSTRACT':abstract, 'EPI_PROB':epi_prob, 'IsEpi':isEpi})\n",
    "            #Slow dataframe update\n",
    "            results = results.append(extraction, ignore_index=True)\n",
    "            \n",
    "    return results.sort_values('EPI_PROB', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd2d51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "GARD_dict, max_length = load_GARD_diseases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a084a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NER_pipeline, labels = init_NER_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea45ad4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp, nlpSci, nlpSci2, classify_model, classify_tokenizer = classify_abs.init_classify_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf4d146",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(term,num_results = 50):\n",
    "    return search_term_extraction(term, num_results, NER_pipeline, labels, GARD_dict, max_length, nlp, nlpSci, nlpSci2, classify_model, classify_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3c0470",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c = search('Fellman syndrome')\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5474a9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict({'LOC':[set(),set(),set(),set()],'DIS':[set(),set(),{'wow'},set()],'NEW':[['dang','fire'],list(),['this','is','a','lot'],[]],'woah':[[],[],[],[]]})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b2bec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.replace(to_replace=[list(),set()], value=np.NaN, inplace=False)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d050a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = PMID_extraction(34449519, NER_pipeline, labels, GARD_dict, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e420df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3f65a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe09b60",
   "metadata": {},
   "source": [
    "previous iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55794f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are the main three main functions that can be called in a noteboook.\n",
    "#Extracts Disease GARD ID, Disease Name, Location, Epidemiologic Identifier, Epidemiologic Statistic given a PubMed ID\n",
    "def PMID_extraction(pmid, NER_pipeline, GARD_dict, max_length):\n",
    "    text = classify_abs.PMID_getAb(pmid)\n",
    "    if len(text)>5:\n",
    "        sentences = str2sents(text)\n",
    "        model_outputs = [NER_pipeline(sent) for sent in sentences]\n",
    "        ab_ids, ab_dis, ab_locs, ab_epis, ab_stats = parse_info(sentences, model_outputs, GARD_dict, max_length)\n",
    "        return text, ab_ids, ab_dis, ab_locs, ab_epis, ab_stats\n",
    "    else:\n",
    "        return '*ABSTRACT NOT FOUND*',{\"N/A\"},{\"N/A\"},{\"N/A\"},{\"N/A\"},[\"N/A\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7ae47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import codecs\n",
    "\n",
    "def load_GARD_diseases():\n",
    "    diseases = json.load(codecs.open('gard-id-name-synonyms.json', 'r', 'utf-8-sig'))\n",
    "\n",
    "    #keys are going to be disease names, values are going to be the GARD ID, set up this way bc dictionaries are faster lookup than lists\n",
    "    GARD_dict = {}\n",
    "    GARD_firstwds = set()\n",
    "\n",
    "    #Find out what the length of the longest disease name sequence is, of all names and synonyms\n",
    "    max_length = -1\n",
    "    for entry in diseases:\n",
    "        if entry['name'] not in GARD_dict.keys():\n",
    "            s = entry['name'].lower().strip()\n",
    "            if s not in STOPWORDS and len(s)>4:\n",
    "                GARD_dict[s] = entry['gard_id']\n",
    "                #This will increase the false negative rate a little bit, but decrease the false positive rate tremendously\n",
    "                if s.split()[0] not in STOPWORDS:\n",
    "                    GARD_firstwds.add(s.split()[0])\n",
    "                #compare length\n",
    "                l = len(s.split())\n",
    "                if l>max_length:\n",
    "                    max_length = l\n",
    "        if entry['synonyms']:\n",
    "            for synonym in entry['synonyms']:\n",
    "                if synonym not in GARD_dict.keys():\n",
    "                    s = synonym.lower().strip()\n",
    "                    if s not in STOPWORDS and len(s)>4:\n",
    "                        GARD_dict[s] = entry['gard_id']\n",
    "                        #This will increase the false negative rate a little bit, but decrease the false positive rate tremendously\n",
    "                        if s.split()[0] not in STOPWORDS:\n",
    "                            GARD_firstwds.add(s.split()[0])\n",
    "                        #compare length\n",
    "                        l = len(s.split())\n",
    "                        if l>max_length:\n",
    "                            max_length = l\n",
    "    return GARD_dict, max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a040dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GARD.csv d.synonyms has oddly saved string data that cannot be converted directly into a list, this converts that\n",
    "def str2list(string):\n",
    "    string = str(string).replace('[','')\n",
    "    string = string.replace(']','')\n",
    "    string = string.strip()\n",
    "    str_list = string.split(',')\n",
    "    for s in str_list:\n",
    "        s = s.strip()\n",
    "        if s=='nan':\n",
    "            str_list.remove('nan')\n",
    "    return str_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c19d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_GARD_diseases():    \n",
    "    GARD_df = pd.read_csv('GARD.csv')\n",
    "    #Convert d.synonym strings into lists\n",
    "    i=0\n",
    "    for i in range(len(GARD_df['d.synonyms'])):\n",
    "        GARD_df['d.synonyms'][i] = str2list(GARD_df['d.synonyms'][i])\n",
    "    #Set up a new & easier to use list of diseases\n",
    "    rowlist = []\n",
    "    i=0\n",
    "    for i in range(len(GARD_df)):\n",
    "        columnlist=[]\n",
    "        columnlist.append(GARD_df['d.name'][i])\n",
    "        columnlist+=GARD_df['d.synonyms'][i]\n",
    "        rowlist.append(columnlist)\n",
    "\n",
    "    #keys are going to be disease names, values are going to be the GARD ID, set up this way bc dictionaries are faster lookup than lists\n",
    "    GARD_dict = {}\n",
    "    GARD_firstwd_dict = {}\n",
    "\n",
    "    #Find out what the length of the longest disease name sequence is, of all names and synonyms\n",
    "    max_length = -1\n",
    "    for i in range(len(rowlist)):\n",
    "        for j in range(len(rowlist[i])):\n",
    "            if rowlist[i][j] not in GARD_dict.keys():\n",
    "                s = str(rowlist[i][j]).lower().strip()\n",
    "                if len(s.split())>0 and s not in STOPWORDS:\n",
    "                    if len(s.split())==1 and (len(s.split()[0])==1 or s.split()[0] in STOPWORDS):\n",
    "                        #We dont want anything that is one letter or a stopword(if it is one word)\n",
    "                        pass\n",
    "                    else:\n",
    "                        GARD_dict[s] = GARD_df['d.gard_id'][i]\n",
    "                        #GARD_firstwd_dict[s.split()[0]] = GARD_df['d.gard_id'][i]\n",
    "                        #This will increase the false negative rate a little bit, but decrease the false positive rate tremendously\n",
    "                        if s.split()[0] not in STOPWORDS and len(s.split()[0])>1:\n",
    "                            GARD_firstwd_dict[s.split()[0]] = GARD_df['d.gard_id'][i]\n",
    "                #compare length\n",
    "                l = len(s.split())\n",
    "                if l>max_length:\n",
    "                    max_length = l\n",
    "    return GARD_dict, max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b527a1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_diseases(sentence, GARD_dict, max_length):   \n",
    "    tokens = [s.strip() for s in nltk.word_tokenize(sentence)]\n",
    "    diseases, ids = [],[]\n",
    "    i=0\n",
    "    while i <len(tokens):  \n",
    "        if (len(tokens)-i) < max_length:\n",
    "            compare_length=len(tokens)-i\n",
    "        else:\n",
    "            compare_length = max_length\n",
    "        #Compares longest sequences first and goes down until there is a match\n",
    "        #print('(start compare_length)',compare_length)\n",
    "        exit = False\n",
    "        while compare_length>0:\n",
    "            s = ' '.join(tokens[i:i+compare_length])\n",
    "            for key in GARD_dict.keys():\n",
    "                if key==s.lower():\n",
    "                    #print('MATCH',s)\n",
    "                    diseases.append(s)\n",
    "                    ids.append(GARD_dict[key])\n",
    "                    #Need to skip over the next few indexes\n",
    "                    i+=compare_length-1\n",
    "                    exit = True #this allows you to break out of two loops\n",
    "                    break\n",
    "            #break out of loop in case there are multiple rare diseases in the same sentence\n",
    "            if exit:\n",
    "                break\n",
    "            else:\n",
    "                compare_length-=1\n",
    "        i+=1  \n",
    "    return diseases,ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f98fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str2sents(string):\n",
    "    for in_sent, replacement in regex_subs.items():\n",
    "        string = in_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5822347f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str2sents(string):\n",
    "    string = re.sub('<.{1,4}>', ' ', string)\n",
    "    string = re.sub(\"  *\", \" \" , string)\n",
    "    string = re.sub(\"^ \", \"\" , string)\n",
    "    string = re.sub(\"$\", \"\" , string)\n",
    "    string = re.sub(\"™\", \"\" , string)\n",
    "    string = re.sub(\"®\", \"\" , string)\n",
    "    string = re.sub(\"•\", \"\" , string)\n",
    "    string = re.sub(\"…\", \"\" , string)\n",
    "    string = re.sub(\"♀\", \"female\" , string)\n",
    "    string = re.sub(\"♂\", \"male\" , string)\n",
    "    string = re.sub(\"α\", \"[alpha]\" , string)\n",
    "    string = re.sub(\"β\", \"[beta]\" , string)\n",
    "    string = re.sub(\"γ\", \"[gamma]\" , string)\n",
    "    string = re.sub(\"δ\", \"[delta]\" , string)\n",
    "    string = re.sub(\"ε\", \"[epsilon]\" , string)\n",
    "    string = re.sub(\"ζ\", \"[zeta]\" , string)\n",
    "    string = re.sub(\"η\", \"[eta]\" , string)\n",
    "    string = re.sub(\"θ\", \"[theta]\" , string)\n",
    "    string = re.sub(\"ι\", \"[iota]\" , string)\n",
    "    string = re.sub(\"κ\", \"[kappa]\" , string)\n",
    "    string = re.sub(\"λ\", \"[lambda]\" , string)\n",
    "    string = re.sub(\"μ\", \"[mu]\" , string)\n",
    "    string = re.sub(\"ν\", \"[nu]\" , string)\n",
    "    string = re.sub(\"ξ\", \"[xi]\" , string)\n",
    "    string = re.sub(\"ο\", \"[omicron]\" , string)\n",
    "    string = re.sub(\"π\", \"[pi]\" , string)\n",
    "    string = re.sub(\"ρ\", \"[rho]\" , string)\n",
    "    string = re.sub(\"σ\", \"[sigma]\" , string)\n",
    "    string = re.sub(\"ς\", \"[sigma]\" , string)\n",
    "    string = re.sub(\"τ\", \"[tau]\" , string)\n",
    "    string = re.sub(\"υ\", \"[upsilon]\" , string)\n",
    "    string = re.sub(\"φ\", \"[phi]\" , string)\n",
    "    string = re.sub(\"χ\", \"[chi]\" , string)\n",
    "    string = re.sub(\"ψ\", \"[psi]\" , string)\n",
    "    string = re.sub(\"ω\", \"[omega]\" , string)\n",
    "    string = unidecode(string)\n",
    "    string=string.strip()\n",
    "    sentences = tokenize.sent_tokenize(string)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec68b615",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_NER_pipeline(path_to_model_folder = \"./NER/outputLG7/\"):\n",
    "    config = BertConfig.from_json_file(str(path_to_model_folder+'config.json'))\n",
    "    tokenizer = BertTokenizer.from_pretrained(path_to_model_folder)\n",
    "    custommodel = AutoModelForTokenClassification.from_pretrained(path_to_model_folder,config=config,local_files_only=True)\n",
    "    customNER = pipeline('ner', custommodel, config=config,tokenizer=tokenizer,aggregation_strategy='simple')\n",
    "    return customNER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27c5430",
   "metadata": {},
   "outputs": [],
   "source": [
    "GARD_dict, max_length = load_GARD_diseases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cb1324",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Can search by 7-digit GARD_ID, 12-digit \"GARD:{GARD_ID}\", matched search term, or arbitrary search term\n",
    "#Returns list of terms to search by\n",
    "def autosearch(searchterm, GARD_dict, matching=2):\n",
    "    while matching>=1:\n",
    "        if 'GARD:' in searchterm and len(searchterm)==12:\n",
    "            return [k for k,v in GARD_dict.items() if v==searchterm]\n",
    "        \n",
    "        elif len(searchterm)==7 and searchterm[0].isdigit() and searchterm[-1].isdigit():\n",
    "            searchterm = 'GARD:'+searchterm\n",
    "            return [k for k,v in GARD_dict.items() if v==searchterm]\n",
    "        \n",
    "        elif searchterm in GARD_dict.keys():\n",
    "            return [k for k,v in GARD_dict.items() if v==GARD_dict[searchterm]]\n",
    "        \n",
    "        else:\n",
    "            searchterm = searchterm.replace(' ','-')\n",
    "            return autosearch(searchterm, GARD_dict, matching-1)\n",
    "    print(\"SEARCH TERM DID NOT MATCH TO GARD DICTIONARY. SEARCHING BY USER INPUT\")\n",
    "    return [searchterm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84f0f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "searchterm_list = 'tay sachs'\n",
    "print(searchterm_list)\n",
    "#type validation, allows string or list input\n",
    "if type(searchterm_list)!=list:\n",
    "    if type(searchterm_list)==str:\n",
    "        searchterm_list = [searchterm_list]\n",
    "    else:\n",
    "        searchterm_list = list(searchterm_list)\n",
    "print(searchterm_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f18e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "autosearch('tay sachs',GARD_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbe79bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "extract_abs.autosearch('Tay-Sachs Disease',GARD_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442df851",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_autosearch_API(searchterm_list, maxResults):\n",
    "    i = 0\n",
    "    pmids = set()\n",
    "    pmid_abs = {}\n",
    "    \n",
    "    for dz in searchterm_list:\n",
    "        term = ''\n",
    "        dz_words = dz.split()\n",
    "        for word in dz_words:\n",
    "            term += word + '%20'\n",
    "        query = term[:-3]\n",
    "\n",
    "        ## get results from searching for disease name through PubMed API\n",
    "        url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term='+query\n",
    "        r = requests.get(url)\n",
    "        root = ET.fromstring(r.content)\n",
    "\n",
    "        # loop over resulting articles\n",
    "        for result in root.iter('IdList'):\n",
    "            if i >= maxResults:\n",
    "                break\n",
    "            pmidlist = [pmid.text for pmid in result.iter('Id')]\n",
    "            pmids.update(pmidlist)\n",
    "            i+=len(pmidlist)\n",
    "\n",
    "        ## get results from searching for disease name through EBI API\n",
    "        url = 'https://www.ebi.ac.uk/europepmc/webservices/rest/search?query='+query+'&resulttype=core'\n",
    "        r = requests.get(url)\n",
    "        root = ET.fromstring(r.content)\n",
    "\n",
    "        # loop over resulting articles\n",
    "        for result in root.iter('result'):\n",
    "            if i >= maxResults:\n",
    "                break\n",
    "            pmidlist = [pmid.text for pmid in result.iter('id')]\n",
    "            if len(pmidlist) > 0:\n",
    "                pmid = pmidlist[0]\n",
    "                if pmid[0].isdigit():\n",
    "                    pmids.add(pmid)\n",
    "                    i += 1\n",
    "\n",
    "    ## get abstracts from EBI PMID API and output a dictionary\n",
    "    for pmid in pmids:\n",
    "        abstract = classify_abs.PMID_getAb(pmid)\n",
    "        if len(abstract)>5:\n",
    "            pmid_abs[pmid] = abstract\n",
    "    \n",
    "    return pmid_abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5b3037",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_autosearch_API(searchterm_list, maxResults):\n",
    "    i = 0\n",
    "    pmids = set()\n",
    "    pmid_abs = {}\n",
    "    \n",
    "    for dz in searchterm_list:\n",
    "        term = ''\n",
    "        dz_words = dz.split()\n",
    "        for word in dz_words:\n",
    "            term += word + '%20'\n",
    "        query = term[:-3]\n",
    "\n",
    "        ## get results from searching for disease name through PubMed API\n",
    "        url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term='+query\n",
    "        r = requests.get(url)\n",
    "        root = ET.fromstring(r.content)\n",
    "\n",
    "        # loop over resulting articles\n",
    "        for result in root.iter('IdList'):\n",
    "            if i >= maxResults:\n",
    "                break\n",
    "            pmidlist = [pmid.text for pmid in result.iter('Id')]\n",
    "            pmids.update(pmidlist)\n",
    "            i+=len(pmidlist)\n",
    "\n",
    "        ## get results from searching for disease name through EBI API\n",
    "        url = 'https://www.ebi.ac.uk/europepmc/webservices/rest/search?query='+query+'&resulttype=core'\n",
    "        r = requests.get(url)\n",
    "        root = ET.fromstring(r.content)\n",
    "\n",
    "        # loop over resulting articles\n",
    "        for result in root.iter('result'):\n",
    "            if i >= maxResults:\n",
    "                break\n",
    "            pmidlist = [pmid.text for pmid in result.iter('id')]\n",
    "            if len(pmidlist) > 0:\n",
    "                pmid = pmidlist[0]\n",
    "                if pmid[0].isdigit():\n",
    "                    pmids.add(pmid)\n",
    "                    i += 1\n",
    "\n",
    "    ## get abstracts from EBI PMID API and output a dictionary\n",
    "    for pmid in pmids:\n",
    "        abstract = classify_abs.PMID_getAb(pmid)\n",
    "        if len(abstract)>5:\n",
    "            pmid_abs[pmid] = abstract\n",
    "    \n",
    "    return pmid_abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c8436a",
   "metadata": {},
   "outputs": [],
   "source": [
    "termlist = autosearch('GARD:0007737', GARD_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213d3af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "termlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5792ef22",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = combined_autosearch_API(termlist, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b99bd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2347a2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9b8088",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAbs(PMID):\n",
    "    url = 'https://www.ebi.ac.uk/europepmc/webservices/rest/search?query=EXT_ID:'+str(PMID)+'&resulttype=core'\n",
    "    r = requests.get(url)\n",
    "    root = ET.fromstring(r.content)\n",
    "    titles = [title.text for title in root.iter('title')]\n",
    "    abstracts = [abstract.text for abstract in root.iter('abstractText')]\n",
    "    if len(abstracts) > 0 and len(abstracts[0])>5:\n",
    "        return titles[0]+' '+abstracts[0]\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713e934a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pubmed_API(searchterm, maxResults):\n",
    "    # get results from searching for disease name through EBI API\n",
    "    term = ''\n",
    "    dz_words = searchterm.split()\n",
    "    for word in dz_words:\n",
    "        term += word + '%20'\n",
    "    query = term[:-3]\n",
    "    url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term='+query\n",
    "    r = requests.get(url)\n",
    "    root = ET.fromstring(r.content)\n",
    "\n",
    "    pmids = []\n",
    "    i = 0\n",
    "\n",
    "    # loop over resulting articles\n",
    "    for result in root.iter('IdList'):\n",
    "        if i >= maxResults:\n",
    "            break\n",
    "        pmids = [pmid.text for pmid in result.iter('Id')]\n",
    "    \n",
    "    pmid_to_abs = {}\n",
    "    for pmid in pmids:\n",
    "        abstract = classify_abs.PMID_getAb(pmid)\n",
    "        if len(abstract)>5:\n",
    "            pmid_to_abs[pmid]=abstract\n",
    "    \n",
    "    return pmid_to_abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd9d0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EBI_API(searchterm, maxResults):\n",
    "    # get results from searching for disease name through EBI API\n",
    "    term = ''\n",
    "    dz_words = searchterm.split()\n",
    "    for word in dz_words:\n",
    "        term += word + '%20'\n",
    "    query = term[:-3]\n",
    "    print('query',query)\n",
    "    url = 'https://www.ebi.ac.uk/europepmc/webservices/rest/search?query='+query+'&resulttype=core'\n",
    "    print('url',url)\n",
    "    r = requests.get(url)\n",
    "    root = ET.fromstring(r.content)\n",
    "\n",
    "    pmid_to_abs = {}\n",
    "    i = 0\n",
    "\n",
    "    # loop over resulting articles\n",
    "    for result in root.iter('result'):\n",
    "        if i >= maxResults:\n",
    "            break\n",
    "        pmids = [pmid.text for pmid in result.iter('id')]\n",
    "        if len(pmids) > 0:\n",
    "            pmid = pmids[0]\n",
    "            if pmid[0].isdigit():\n",
    "                abstracts = [abstract.text for abstract in result.iter('abstractText')]\n",
    "                if len(abstracts) > 0:\n",
    "                    pmid_to_abs[pmid] = abstracts[0]\n",
    "                    i += 1\n",
    "    return pmid_to_abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe98ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_API(searchterm, maxResults):\n",
    "    term = ''\n",
    "    dz_words = searchterm.split()\n",
    "    for word in dz_words:\n",
    "        term += word + '%20'\n",
    "    query = term[:-3]\n",
    "    i = 0\n",
    "    pmids = set()\n",
    "    \n",
    "    ## get results from searching for disease name through PubMed API\n",
    "    url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term='+query\n",
    "    r = requests.get(url)\n",
    "    root = ET.fromstring(r.content)\n",
    "    \n",
    "    # loop over resulting articles\n",
    "    for result in root.iter('IdList'):\n",
    "        if i >= maxResults:\n",
    "            break\n",
    "        pmids = {pmid.text for pmid in result.iter('Id')}\n",
    "    \n",
    "    ## get results from searching for disease name through EBI API\n",
    "    url = 'https://www.ebi.ac.uk/europepmc/webservices/rest/search?query='+query+'&resulttype=core'\n",
    "    r = requests.get(url)\n",
    "    root = ET.fromstring(r.content)\n",
    "    \n",
    "    # loop over resulting articles\n",
    "    for result in root.iter('result'):\n",
    "        if i >= maxResults:\n",
    "            break\n",
    "        pmidlist = [pmid.text for pmid in result.iter('id')]\n",
    "        if len(pmidlist) > 0:\n",
    "            pmid = pmidlist[0]\n",
    "            if pmid[0].isdigit():\n",
    "                pmids.add(pmid)\n",
    "                i += 1\n",
    "    \n",
    "    ## get abstracts from EBI PMID API and output a dictionary\n",
    "    pmid_abs = {}\n",
    "    for pmid in pmids:\n",
    "        abstract = classify_abs.PMID_getAb(pmid)\n",
    "        if len(abstract)>5:\n",
    "            pmid_abs[pmid] = abstract\n",
    "    \n",
    "    return pmid_abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b81377",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combined API, but optimized to have fewer API calls\n",
    "def optimized_API(search_term,maxResults): #returns a dictionary of {pmids:abstracts}    \n",
    "    term = ''\n",
    "    dz_words = search_term.split()\n",
    "    for word in dz_words:\n",
    "        term += word + '%20'\n",
    "    query = term[:-3]\n",
    "    url = 'https://www.ebi.ac.uk/europepmc/webservices/rest/search?query='+query+'&resulttype=core'\n",
    "    r = requests.get(url)\n",
    "    root = ET.fromstring(r.content)\n",
    "\n",
    "    pmids_abs = {}\n",
    "    i = 0\n",
    "\n",
    "    # loop over resulting articles\n",
    "    for result in root.iter('result'):\n",
    "        if i >= maxResults:\n",
    "            break\n",
    "        pmids = [pmid.text for pmid in result.iter('id')]\n",
    "        if len(pmids) > 0:\n",
    "            pmid = pmids[0]\n",
    "            if pmid[0].isdigit():\n",
    "                abstracts = [abstract.text for abstract in result.iter('abstractText')]\n",
    "                titles = [title.text for title in result.iter('title')]\n",
    "                if len(abstracts) > 0 and len(abstracts[0])>5:\n",
    "                    pmids_abs[pmid] = titles[0]+' '+abstracts[0]\n",
    "                    i+=1\n",
    "    \n",
    "    #PubMed API gets different results\n",
    "    url2 = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term='+query\n",
    "    r2 = requests.get(url2)\n",
    "    root2 = ET.fromstring(r2.content)\n",
    "    \n",
    "    for result in root.iter('IdList'):\n",
    "        if i >= maxResults:\n",
    "            break\n",
    "        pmids = [pmid.text for pmid in result.iter('Id')]\n",
    "        i+=len(pmids)\n",
    "        for pmid in pmids:\n",
    "            if pmid not in pmids_abs.keys():\n",
    "                abstract = classify_abs.PMID_getAb(pmid)\n",
    "                if len(abstract)>5:\n",
    "                    pmids_abs[pmid]=abstract\n",
    "     \n",
    "    return pmids_abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3714d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEPRECATED, old function, only takes in string input for search term\n",
    "## Gets results from searching through both PubMed and EBI search term APIs, also makes use of the EBI API for PMIDs. \n",
    "## EBI API and PubMed API give different results\n",
    "# This makes n+2 API calls where n<=maxResults, which is slow \n",
    "# There is a way to optimize by gathering abstracts from the EBI API when also getting pmids but did not pursue due to time constraints\n",
    "def search_getAbs(searchterm, maxResults):\n",
    "    term = ''\n",
    "    dz_words = searchterm.split()\n",
    "    for word in dz_words:\n",
    "        term += word + '%20'\n",
    "    query = term[:-3]\n",
    "    i = 0\n",
    "    pmids = set()\n",
    "    \n",
    "    ## get results from searching for disease name through PubMed API\n",
    "    url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term='+query\n",
    "    r = requests.get(url)\n",
    "    root = ET.fromstring(r.content)\n",
    "    \n",
    "    # loop over resulting articles\n",
    "    for result in root.iter('IdList'):\n",
    "        if i >= maxResults:\n",
    "            break\n",
    "        pmids = {pmid.text for pmid in result.iter('Id')}\n",
    "    \n",
    "    ## get results from searching for disease name through EBI API\n",
    "    url = 'https://www.ebi.ac.uk/europepmc/webservices/rest/search?query='+query+'&resulttype=core'\n",
    "    r = requests.get(url)\n",
    "    root = ET.fromstring(r.content)\n",
    "    \n",
    "    # loop over resulting articles\n",
    "    for result in root.iter('result'):\n",
    "        if i >= maxResults:\n",
    "            break\n",
    "        pmidlist = [pmid.text for pmid in result.iter('id')]\n",
    "        if len(pmidlist) > 0:\n",
    "            pmid = pmidlist[0]\n",
    "            if pmid[0].isdigit():\n",
    "                pmids.add(pmid)\n",
    "                i += 1\n",
    "    \n",
    "    ## get abstracts from EBI PMID API and output a dictionary\n",
    "    pmid_abs = {}\n",
    "    for pmid in pmids:\n",
    "        abstract = PMID_getAb(pmid)\n",
    "        if len(abstract)>5:\n",
    "            pmid_abs[pmid] = abstract\n",
    "    \n",
    "    return pmid_abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dfe073",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ccb50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PMID_extraction(pmid, NER_pipeline, GARD_dict, max_length):\n",
    "    text = getAbs(pmid)\n",
    "    if len(text)>5:\n",
    "        sentences = str2sents(text)\n",
    "        results = [NER_pipeline(sent) for sent in sentences]\n",
    "        ab_ids, ab_dis, ab_locs, ab_epis, ab_stats = parse_info(sentences, results)\n",
    "        return text, ab_ids, ab_dis, ab_locs, ab_epis, ab_stats\n",
    "    else:\n",
    "        return '*ABSTRACT NOT FOUND*',{\"N/A\"},{\"N/A\"},{\"N/A\"},{\"N/A\"},{\"N/A\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dd6097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_extraction(text, NER_pipeline, GARD_dict, max_length):\n",
    "    if len(text)>5:\n",
    "        sentences = str2sents(text)\n",
    "        results = [NER_pipeline(sent) for sent in sentences]\n",
    "        ab_ids, ab_dis, ab_locs, ab_epis, ab_stats = parse_info(sentences, results)\n",
    "        return text, ab_ids, ab_dis, ab_locs, ab_epis, ab_stats\n",
    "    else:\n",
    "        return '*Text too short*',{\"N/A\"},{\"N/A\"},{\"N/A\"},{\"N/A\"},{\"N/A\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611a592c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "customNER = init_NER_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b56579",
   "metadata": {},
   "outputs": [],
   "source": [
    "GARD_dict, max_length = load_GARD_diseases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f493b073",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "text, ab_ids, ab_dis, ab_locs, ab_epis, ab_stats = PMID_extraction(25274184, customNER, GARD_dict, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5649ec",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(text,\n",
    "      '\\n\\nGARD Disease ID: ',ab_ids, \n",
    "      '\\nGARD Disease: ',ab_dis, \n",
    "      '\\nLocations: ',ab_locs, \n",
    "      '\\nEpi Identifier: ',ab_epis, \n",
    "      '\\nEpi Statistics: ',ab_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2f8cb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    print('Loading NER Pipeline..')\n",
    "    path_to_model_folder = input('Input path_to_model_folder. Input \"d\" to use default model.')\n",
    "    if path_to_model_folder == 'd':\n",
    "        NER_pipeline = init_NER_pipeline()\n",
    "    else:\n",
    "        NER_pipeline = init_NER_pipeline(path_to_model_folder)\n",
    "    print('NER Pipeline Loaded')\n",
    "    \n",
    "    print('Loading GARD Disease Dictionary....')\n",
    "    GARD_dict, GARD_firstwd_dict, max_length = load_GARD_diseases()\n",
    "    print('Loading GARD Diseases Loaded')\n",
    "    pmid = input('\\nEnter PubMed PMID (or DONE): ')\n",
    "    while pmid != 'DONE':\n",
    "        text, ab_ids, ab_dis, ab_locs, ab_epis, ab_stats = PMID_extraction(pmid, NER_pipeline, GARD_dict, max_length)\n",
    "        print('GARD Disease ID: ',ab_ids, \n",
    "              '\\nGARD Disease: ',ab_dis, \n",
    "              '\\nLocations: ',ab_locs, \n",
    "              '\\nEpi Identifier: ',ab_epis, \n",
    "              '\\nEpi Statistics: ',ab_stats)\n",
    "        pmid = input('\\nEnter PubMed PMID (or DONE): ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcd43f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ff879c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "import re\n",
    "def filter_results(searchterm_list,pmid_abs):\n",
    "    \n",
    "    terms = set(searchterm_list).union(set(str(re.sub(',','',' '.join(searchterm_list))).split()).difference(STOPWORDS))\n",
    "    '''\n",
    "    joined = ' '.join(searchterm_list)\n",
    "    comma_gone = re.sub(',','',joined)\n",
    "    split = set(comma_gone.split())\n",
    "    key_words = split.difference(STOPWORDS)\n",
    "    search_set = set(searchterm_list)\n",
    "    terms = search_set.union(key_words)\n",
    "    '''\n",
    "    print(len(terms),terms)\n",
    "    print()\n",
    "    for abstract in pmid_abs.values():\n",
    "        for term in terms:\n",
    "            print(term)\n",
    "            if term in abstract:\n",
    "                print('yes')\n",
    "            else:\n",
    "                print('no')\n",
    "            print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f63e1b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "searchterm_list = ['facioscapulohumeral muscular dystrophy', 'muscular dystrophy, facioscapulohumeral', 'facioscapulohumeral muscular dystrophy 1a', 'fshmd1a', 'muscular dystrophy, facioscapulohumeral, type 1a', 'fshd1a', 'landouzy-dejerine muscular dystrophy']\n",
    "import extract_abs\n",
    "GARD_dict, max_length = extract_abs.load_GARD_diseases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e31f1f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "searchterm_list ['facioscapulohumeral muscular dystrophy', 'muscular dystrophy, facioscapulohumeral', 'facioscapulohumeral muscular dystrophy 1a', 'fshmd1a', 'muscular dystrophy, facioscapulohumeral, type 1a', 'fshd1a', 'landouzy-dejerine muscular dystrophy']\n",
      "\n",
      "JOINED facioscapulohumeral muscular dystrophy muscular dystrophy, facioscapulohumeral facioscapulohumeral muscular dystrophy 1a fshmd1a muscular dystrophy, facioscapulohumeral, type 1a fshd1a landouzy-dejerine muscular dystrophy\n",
      "\n",
      "comma_gone facioscapulohumeral muscular dystrophy muscular dystrophy facioscapulohumeral facioscapulohumeral muscular dystrophy 1a fshmd1a muscular dystrophy facioscapulohumeral type 1a fshd1a landouzy-dejerine muscular dystrophy\n",
      "\n",
      "split {'fshmd1a', 'landouzy-dejerine', '1a', 'dystrophy', 'facioscapulohumeral', 'fshd1a', 'type', 'muscular'}\n",
      "\n",
      "STOPWORDS {'during', 'm', 'aren', 'when', 'which', 'itself', 'mightn', 'shouldn', 'themselves', 'was', \"mustn't\", 'wasn', 'at', \"haven't\", 'you', 'him', 'yours', 'as', 'too', \"needn't\", 'own', 'needn', \"isn't\", 'theirs', 'an', 'out', 'will', 'were', 'having', \"hasn't\", 'hasn', 'are', \"don't\", 'some', 'their', \"she's\", 'didn', 'there', 'we', 'with', 'just', 'a', 'such', 'all', 'other', \"you'd\", 'into', \"wasn't\", 'any', \"doesn't\", 'above', \"weren't\", 'himself', 'in', 'most', 'd', 'll', 'what', 'few', 'off', 'before', \"couldn't\", 'my', 'yourselves', \"aren't\", 'can', 'being', 'on', 'until', 'mustn', 'to', \"didn't\", 'hadn', 'how', \"mightn't\", 'no', 'his', 'if', 'he', 'from', 'over', 'between', 'more', 'wouldn', 'where', 'o', \"should've\", 'but', 'after', \"wouldn't\", 'those', 'ma', 'of', 'ours', 'hers', 'i', \"you've\", 'here', 'both', 'she', 'her', 'while', \"that'll\", 'shan', 'then', 'have', 'by', 'once', 'should', 'its', 'nor', 'only', 'again', 'each', 'me', 'whom', 'against', \"you'll\", 'and', 'below', 'doesn', \"hadn't\", 't', 'further', 'them', 'won', 'who', 'herself', 're', \"shouldn't\", \"you're\", 'or', 'these', 'down', 'this', 'your', 'because', 'do', 'through', 'why', 'am', 'myself', 'doing', 'had', \"won't\", 'is', 'they', 'been', 'same', 'has', 'now', 'couldn', 'our', 'be', \"shan't\", 'very', 'so', 's', 'it', 'up', 'under', 'not', 'ourselves', 'than', 'the', 'about', 've', 'ain', 'don', 'isn', 'yourself', \"it's\", 'haven', 'for', 'did', 'y', 'weren', 'does', 'that'}\n",
      "\n",
      "key_words {'fshmd1a', 'landouzy-dejerine', '1a', 'facioscapulohumeral', 'dystrophy', 'fshd1a', 'type', 'muscular'}\n",
      "\n",
      "{'muscular dystrophy, facioscapulohumeral, type 1a', 'fshmd1a', 'landouzy-dejerine muscular dystrophy', 'facioscapulohumeral muscular dystrophy', 'fshd1a', 'muscular dystrophy, facioscapulohumeral', 'facioscapulohumeral muscular dystrophy 1a'}\n",
      "terms 13 {'muscular dystrophy, facioscapulohumeral, type 1a', 'fshmd1a', 'landouzy-dejerine muscular dystrophy', 'facioscapulohumeral muscular dystrophy', 'landouzy-dejerine', '1a', 'facioscapulohumeral', 'fshd1a', 'dystrophy', 'muscular dystrophy, facioscapulohumeral', 'type', 'facioscapulohumeral muscular dystrophy 1a', 'muscular'}\n",
      "\n",
      "muscular dystrophy, facioscapulohumeral, type 1a\n",
      "no\n",
      "\n",
      "fshmd1a\n",
      "no\n",
      "\n",
      "landouzy-dejerine muscular dystrophy\n",
      "no\n",
      "\n",
      "facioscapulohumeral muscular dystrophy\n",
      "yes\n",
      "\n",
      "landouzy-dejerine\n",
      "no\n",
      "\n",
      "1a\n",
      "no\n",
      "\n",
      "facioscapulohumeral\n",
      "yes\n",
      "\n",
      "fshd1a\n",
      "no\n",
      "\n",
      "dystrophy\n",
      "yes\n",
      "\n",
      "muscular dystrophy, facioscapulohumeral\n",
      "no\n",
      "\n",
      "type\n",
      "yes\n",
      "\n",
      "facioscapulohumeral muscular dystrophy 1a\n",
      "no\n",
      "\n",
      "muscular\n",
      "yes\n",
      "\n",
      "muscular dystrophy, facioscapulohumeral, type 1a\n",
      "no\n",
      "\n",
      "fshmd1a\n",
      "no\n",
      "\n",
      "landouzy-dejerine muscular dystrophy\n",
      "no\n",
      "\n",
      "facioscapulohumeral muscular dystrophy\n",
      "no\n",
      "\n",
      "landouzy-dejerine\n",
      "no\n",
      "\n",
      "1a\n",
      "no\n",
      "\n",
      "facioscapulohumeral\n",
      "no\n",
      "\n",
      "fshd1a\n",
      "no\n",
      "\n",
      "dystrophy\n",
      "no\n",
      "\n",
      "muscular dystrophy, facioscapulohumeral\n",
      "no\n",
      "\n",
      "type\n",
      "no\n",
      "\n",
      "facioscapulohumeral muscular dystrophy 1a\n",
      "no\n",
      "\n",
      "muscular\n",
      "no\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d= {14321:\"Predictors of functional outcomes in patients with facioscapulohumeral muscular dystrophy. Facioscapulohumeral muscular dystrophy (FSHD) is one of the most prevalent muscular dystrophies characterized by considerable variability in severity, rates of progression and functional outcomes. Few studies follow FSHD cohorts long enough to understand predictors of disease progression and functional outcomes, creating gaps in our understanding, which impacts clinical care and the design of clinical trials. Efforts to identify molecularly targeted therapies create a need to better understand disease characteristics with predictive value to help refine clinical trial strategies and understand trial outcomes. Here we analysed a prospective cohort from a large, longitudinally followed registry of patients with FSHD in the USA to determine predictors of outcomes such as need for wheelchair use. This study analysed de-identified data from 578 individuals with confirmed FSHD type 1 enrolled in the United States National Registry for FSHD Patients and Family members. Data were collected from January 2002 to September 2019 and included an average of 9 years (range 0-18) of follow-up surveys. Data were analysed using descriptive epidemiological techniques, and risk of wheelchair use was determined using Cox proportional hazards models. Supervised machine learning analysis was completed using Random Forest modelling and included all 189 unique features collected from registry questionnaires. A separate medications-only model was created that included 359 unique medications reported by participants. Here we show that smaller allele sizes were predictive of earlier age at onset, diagnosis and likelihood of wheelchair use. Additionally, we show that females were more likely overall to progress to wheelchair use and at a faster rate as compared to males, independent of genetics. Use of machine learning models that included all reported clinical features showed that the effect of allele size on progression to wheelchair use is small compared to disease duration, which may be important to consider in trial design. Medical comorbidities and medication use add to the risk for need for wheelchair dependence, raising the possibility for better medical management impacting outcomes in FSHD. The findings in this study will require further validation in additional, larger datasets but could have implications for clinical care, and inclusion criteria for future clinical trials in FSHD.\",\n",
    "34242: \"Promising Perspective to Treatment: Nutraceuticals and Phytochemicals.  (FSHD) is in the top three list of all dystrophies with an approximate 1:8000 incidence. It is not a life-threatening disease; however, progression of the disease extends over being wheel-chair bound. Despite some drug trials have been continuing, including DUX4 inhibition, TGF-ß inhibition and resokine which promote healthier muscle, there is not an applicable treatment option for FSHD today. Still, there is a need for new agent or agents to heal, to stop or at least to slow down the muscle wasting. Current FSHD studies with nutraceuticals as vitamin C, vitamin E, coenzyme Q10, zinc, selenium, and phytochemicals as curcumin or genistein, daidzein flavonoids provide promising treatment strategies. In this review we will present the clinical and molecular nature of FSHD and focus on nutraceuticals and phytochemicals that may alleviate FSHD. Via interconnection of impaired pathophysiological FSHD pathways together with nutraceuticals and phytochemicals in the light of literature, we present both studied and novel approaches that can contribute FSHD treatment.\"}\n",
    "\n",
    "\n",
    "filter_results(searchterm_list,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7372ea73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "import re\n",
    "\n",
    "def filter_results(searchterm_list,pmid_abs):\n",
    "    print('searchterm_list',searchterm_list)\n",
    "    print()\n",
    "    joined = ' '.join(searchterm_list)\n",
    "    print('JOINED',joined)\n",
    "    print()\n",
    "    comma_gone = re.sub(',','',joined)\n",
    "    print(\"comma_gone\",comma_gone)\n",
    "    print()\n",
    "    split = set(comma_gone.split())\n",
    "    print(\"split\",split)\n",
    "    print()\n",
    "    print(\"STOPWORDS\",STOPWORDS)\n",
    "    print()\n",
    "    key_words = split.difference(STOPWORDS)\n",
    "    print(\"key_words\",key_words)\n",
    "    print()\n",
    "    search_set = set(searchterm_list)\n",
    "    print(search_set)\n",
    "    terms = search_set.union(key_words)\n",
    "    print('terms',len(terms),terms)\n",
    "    print()\n",
    "    for abstract in pmid_abs.values():\n",
    "        for term in terms:\n",
    "            print(term)\n",
    "            if term in abstract:\n",
    "                print('yes')\n",
    "            else:\n",
    "                print('no')\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cf4ad2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17d4ddc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Deprecated?\n",
    "'''\n",
    "if __name__ == '__main__':\n",
    "    print('Loading NER Pipeline..')\n",
    "    path_to_model_folder = input('Input path_to_model_folder. Input \"d\" to use default model.')\n",
    "    if path_to_model_folder == 'd':\n",
    "        NER_pipeline = init_NER_pipeline()\n",
    "    else:\n",
    "        NER_pipeline = init_NER_pipeline(path_to_model_folder)\n",
    "    print('NER Pipeline Loaded')\n",
    "    \n",
    "    print('Loading GARD Disease Dictionary....')\n",
    "    GARD_dict, GARD_firstwd_dict, max_length = load_GARD_diseases()\n",
    "    \n",
    "    pmid = input('\\nEnter PubMed PMID (or DONE): ')\n",
    "    while pmid != 'DONE':\n",
    "        text = getAbs(pmid)\n",
    "        if len(text)>5:\n",
    "            sentences = str2sents(text)\n",
    "            results = [NER_pipeline(sent) for sent in sentences]\n",
    "            ab_ids, ab_dis, ab_locs, ab_epis, ab_stats = parse_info(sentences, results)\n",
    "            print(text,\n",
    "            '\\nGARD Disease ID: ',ab_ids, \n",
    "            '\\nGARD Disease: ',ab_dis,\n",
    "            \\nLocations: ',ab_locs, \n",
    "            '\\nEpi Identifier: ',ab_epis, \n",
    "            '\\nEpi Statistics: ',ab_stats)\n",
    "        else:\n",
    "            print(\"Title and abstract not found.\")\n",
    "        pmid = input('\\nEnter PubMed PMID (or DONE): ')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249f690d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deprecated?\n",
    "'''\n",
    "#This function can be sped up by using the GARD_firstwd_dict, but as this function works rn, I will not be implementing.\n",
    "#There are most likely many other ways to optimize this function\n",
    "def get_diseases(sentence, GARD_dict, max_length):   \n",
    "    tokens = [s.lower().strip() for s in nltk.word_tokenize(sentence)]\n",
    "    diseases = []\n",
    "    ids = []\n",
    "    i=0\n",
    "    while i <len(tokens):\n",
    "        if (len(tokens)-i) < max_length:\n",
    "            compare_length=len(tokens)-i\n",
    "        else:\n",
    "            compare_length = max_length\n",
    "        #Compares longest sequences first and goes down until there is a match\n",
    "        #print('(start compare_length)',compare_length)\n",
    "        exit = False\n",
    "        while compare_length>0:\n",
    "            s = ' '.join(tokens[i:i+compare_length])\n",
    "            for key in GARD_dict.keys():\n",
    "                if key==s.lower():\n",
    "                    diseases.append(s)\n",
    "                    ids.append(GARD_dict[key])\n",
    "                    #Need to skip over the next few indexes\n",
    "                    i+=compare_length-1\n",
    "                    exit = True #this allows you to break out of two loops\n",
    "                    break\n",
    "            #break out of loop in case there are multiple rare diseases in the same sentence\n",
    "            if exit:\n",
    "                break\n",
    "            else:\n",
    "                compare_length-=1\n",
    "        i+=1  \n",
    "    return diseases,ids\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e62fc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deprecated\n",
    "'''\n",
    "def tag_diseases(tokens,labels, GARD_dict, GARD_firstwd_dict, max_length):   \n",
    "    i=0\n",
    "    while i <len(tokens):\n",
    "        if (len(tokens)-i) < max_length:\n",
    "            compare_length=len(tokens)-i\n",
    "        else:\n",
    "            compare_length = max_length\n",
    "        #Compares longest sequences first and goes down until there is a match\n",
    "        #print('(start compare_length)',compare_length)\n",
    "        exit = False\n",
    "        while compare_length>0:\n",
    "            s = ' '.join(tokens[i:i+compare_length])\n",
    "            for key in GARD_dict.keys():\n",
    "                if key==s.lower():\n",
    "                    labels[i] = 'B-DIS'\n",
    "                    #print(s)\n",
    "                    for j in range(i+1,i+compare_length):\n",
    "                        labels[j] = 'I-DIS'\n",
    "                    #Need to skip over the next few indexes\n",
    "                    #print('(compare_length):',compare_length)\n",
    "                    i+=compare_length-1\n",
    "                    exit =True #this allows you to break out of two loops\n",
    "                    break\n",
    "            #break out of loop in case there are multiple rare diseases in the same sentence\n",
    "            if exit:\n",
    "                break\n",
    "            else:\n",
    "                compare_length-=1\n",
    "        i+=1  \n",
    "    return tokens,labels\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f871c5a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c42e382",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6df13fae",
   "metadata": {},
   "source": [
    "## Goals\n",
    "Write a program that easily adjusts the labels of the Gold Standard Dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0469b49c",
   "metadata": {},
   "source": [
    "### Importing files to modify tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a09b34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86256ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4426 4426\n"
     ]
    }
   ],
   "source": [
    "epi_train_tokens, epi_train_labels = [],[]\n",
    "with open('epi_gold_train.tsv','r', encoding=\"utf-8\") as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    sentence_tokens,sentences_tags=[],[]\n",
    "    for row in reader:\n",
    "        if len(row)%2==0:\n",
    "            if len(row)==0:\n",
    "                if len(sentence_tokens) != len(sentences_tags):\n",
    "                    print('uh oh', sentence_tokens, sentences_tags, sep='\\n')\n",
    "                epi_train_tokens.append(sentence_tokens.copy())\n",
    "                epi_train_labels.append(sentences_tags.copy())\n",
    "                sentence_tokens.clear()\n",
    "                sentences_tags.clear()\n",
    "            else:\n",
    "                sentence_tokens.append(row[0])\n",
    "                sentences_tags.append(row[1])\n",
    "f.close()\n",
    "print(len(epi_train_tokens),len(epi_train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6746615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1206 1206\n"
     ]
    }
   ],
   "source": [
    "epi_val_tokens, epi_val_labels= [],[]\n",
    "with open('epi_gold_val.tsv','r', encoding=\"utf-8\") as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    sentence_tokens,sentences_tags=[],[]\n",
    "    for row in reader:\n",
    "        if len(row)%2==0:\n",
    "            if len(row)==0:\n",
    "                epi_val_tokens.append(sentence_tokens.copy())\n",
    "                epi_val_labels.append(sentences_tags.copy())\n",
    "                sentence_tokens.clear()\n",
    "                sentences_tags.clear()\n",
    "            else:\n",
    "                sentence_tokens.append(row[0])\n",
    "                sentences_tags.append(row[1])\n",
    "f.close()\n",
    "print(len(epi_val_tokens),len(epi_val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6837ddf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "537 537\n"
     ]
    }
   ],
   "source": [
    "#Need to modify test to calculate labeling accuracy\n",
    "epi_test_tokens, epi_test_labels= [],[]\n",
    "with open('epi_gold_test.tsv','r', encoding=\"utf-8\") as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    sentence_tokens,sentences_tags=[],[]\n",
    "    for row in reader:\n",
    "        if len(row)%2==0:\n",
    "            if len(row)==0:\n",
    "                epi_test_tokens.append(sentence_tokens.copy())\n",
    "                epi_test_labels.append(sentences_tags.copy())\n",
    "                sentence_tokens.clear()\n",
    "                sentences_tags.clear()\n",
    "            else:\n",
    "                sentence_tokens.append(row[0])\n",
    "                sentences_tags.append(row[1])\n",
    "f.close()\n",
    "print(len(epi_test_tokens),len(epi_test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886c79e7",
   "metadata": {},
   "source": [
    "### Replacing ABRV -> DIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7903bec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_abrv_labels(tokens,labels):\n",
    "    if len(tokens)!=len(labels):\n",
    "        raise IndexError(\"Num Sentences {} and Num Sentence Labels {} Mismatch\".format(len(tokens),len(labels)))\n",
    "    \n",
    "    for i in tqdm(range(len(tokens))):\n",
    "        if len(tokens[i])!=len(labels[i]):\n",
    "            raise IndexError(\"Sentence Length {} and Label Length {} Mismatch\".format(len(tokens[i]),len(labels[i])))\n",
    "        for j in range(len(labels[i])):\n",
    "            if labels[i][j]=='B-ABRV':\n",
    "                labels[i][j]=='O'\n",
    "            if labels[i][j]=='I-ABRV':\n",
    "                labels[i][j]=='O'\n",
    "    \n",
    "    return tokens, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6615a8a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4426/4426 [00:00<00:00, 148086.61it/s]\n"
     ]
    }
   ],
   "source": [
    "mod_train_tokens, mod_train_labels = replace_abrv_labels(epi_train_tokens,epi_train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "088737ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1206/1206 [00:00<00:00, 150250.42it/s]\n"
     ]
    }
   ],
   "source": [
    "mod_val_tokens, mod_val_labels = replace_abrv_labels(epi_val_tokens,epi_val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1edf2b88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 537/537 [00:00<00:00, 149309.99it/s]\n"
     ]
    }
   ],
   "source": [
    "mod_test_tokens, mod_test_labels = replace_abrv_labels(epi_test_tokens,epi_test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a701a6a",
   "metadata": {},
   "source": [
    "#### Saving the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e91767e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('epi_gold_train-abrv.tsv', \"w\") as f:\n",
    "    for i in range(len(mod_train_tokens)): #For sentence in list of sentences\n",
    "        for j in range(len(mod_train_tokens[i])): #for token in sentence\n",
    "            output = str(mod_train_tokens[i][j]) +'\\t' +str(mod_train_labels[i][j])+'\\n'\n",
    "            f.write(output)\n",
    "        f.write('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca67ccaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('epi_gold_val-abrv.tsv', \"w\") as f:\n",
    "    for i in range(len(mod_val_tokens)): #For sentence in list of sentences\n",
    "        for j in range(len(mod_val_tokens[i])): #for token in sentence\n",
    "            output = str(mod_val_tokens[i][j]) +'\\t' +str(mod_val_labels[i][j])+'\\n'\n",
    "            f.write(output)\n",
    "        f.write('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e5a5fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('epi_gold_test-abrv.tsv', \"w\") as f:\n",
    "    for i in range(len(mod_test_tokens)): #For sentence in list of sentences\n",
    "        for j in range(len(mod_test_tokens[i])): #for token in sentence\n",
    "            output = str(mod_test_tokens[i][j]) +'\\t' +str(mod_test_labels[i][j])+'\\n'\n",
    "            f.write(output)\n",
    "        f.write('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b8e9b5",
   "metadata": {},
   "source": [
    "### Replacing ABRV -> DIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80e3a7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_abrv_labels(tokens,labels):\n",
    "    if len(tokens)!=len(labels):\n",
    "        raise IndexError(\"Num Sentences {} and Num Sentence Labels {} Mismatch\".format(len(tokens),len(labels)))\n",
    "    \n",
    "    for i in tqdm(range(len(tokens))):\n",
    "        if len(tokens[i])!=len(labels[i]):\n",
    "            raise IndexError(\"Sentence Length {} and Label Length {} Mismatch\".format(len(tokens[i]),len(labels[i])))\n",
    "        for j in range(len(labels[i])):\n",
    "            if labels[i][j]=='B-ABRV':\n",
    "                labels[i][j]=='B-DIS'\n",
    "            if labels[i][j]=='I-ABRV':\n",
    "                labels[i][j]=='I-DIS'\n",
    "    \n",
    "    return tokens, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c4b264f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4426/4426 [00:00<00:00, 145529.15it/s]\n"
     ]
    }
   ],
   "source": [
    "mod_train_tokens, mod_train_labels = replace_abrv_labels(epi_train_tokens,epi_train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec0e0bef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1206/1206 [00:00<00:00, 145224.96it/s]\n"
     ]
    }
   ],
   "source": [
    "mod_val_tokens, mod_val_labels = replace_abrv_labels(epi_val_tokens,epi_val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c61f7aaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 537/537 [00:00<00:00, 142094.58it/s]\n"
     ]
    }
   ],
   "source": [
    "mod_test_tokens, mod_test_labels = replace_abrv_labels(epi_test_tokens,epi_test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be2a1aa",
   "metadata": {},
   "source": [
    "#### Saving the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8cfc1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('epi_gold_train_abrv>dis.tsv', \"w\") as f:\n",
    "    for i in range(len(mod_train_tokens)): #For sentence in list of sentences\n",
    "        for j in range(len(mod_train_tokens[i])): #for token in sentence\n",
    "            output = str(mod_train_tokens[i][j]) +'\\t' +str(mod_train_labels[i][j])+'\\n'\n",
    "            f.write(output)\n",
    "        f.write('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f33eada",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('epi_gold_val_abrv>dis.tsv', \"w\") as f:\n",
    "    for i in range(len(mod_val_tokens)): #For sentence in list of sentences\n",
    "        for j in range(len(mod_val_tokens[i])): #for token in sentence\n",
    "            output = str(mod_val_tokens[i][j]) +'\\t' +str(mod_val_labels[i][j])+'\\n'\n",
    "            f.write(output)\n",
    "        f.write('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3d2ba38",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('epi_gold_test_abrv>dis.tsv', \"w\") as f:\n",
    "    for i in range(len(mod_test_tokens)): #For sentence in list of sentences\n",
    "        for j in range(len(mod_test_tokens[i])): #for token in sentence\n",
    "            output = str(mod_test_tokens[i][j]) +'\\t' +str(mod_test_labels[i][j])+'\\n'\n",
    "            f.write(output)\n",
    "        f.write('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565ceb15",
   "metadata": {},
   "source": [
    "### Removing DIS & ABRV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ff4d56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_disease_labels(tokens,labels):\n",
    "    if len(tokens)!=len(labels):\n",
    "        raise IndexError(\"Num Sentences {} and Num Sentence Labels {} Mismatch\".format(len(tokens),len(labels)))\n",
    "    \n",
    "    for i in tqdm(range(len(tokens))):\n",
    "        if len(tokens[i])!=len(labels[i]):\n",
    "            raise IndexError(\"Sentence Length {} and Label Length {} Mismatch\".format(len(tokens[i]),len(labels[i])))\n",
    "        for j in range(len(labels[i])):\n",
    "            if labels[i][j] in {'B-ABRV','I-ABRV','B-DIS','I-DIS'}:\n",
    "                labels[i][j]='O'\n",
    "    \n",
    "    return tokens, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd6800d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4426/4426 [00:00<00:00, 227399.55it/s]\n"
     ]
    }
   ],
   "source": [
    "mod_train_tokens, mod_train_labels = remove_disease_labels(epi_train_tokens,epi_train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "931183a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1206/1206 [00:00<00:00, 208290.33it/s]\n"
     ]
    }
   ],
   "source": [
    "mod_val_tokens, mod_val_labels = remove_disease_labels(epi_val_tokens,epi_val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c50a0c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 537/537 [00:00<00:00, 230442.12it/s]\n"
     ]
    }
   ],
   "source": [
    "mod_test_tokens, mod_test_labels = remove_disease_labels(epi_test_tokens,epi_test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67394fa",
   "metadata": {},
   "source": [
    "#### Saving the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32ce555f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('epi_gold_train-dz.tsv', \"w\") as f:\n",
    "    for i in range(len(mod_train_tokens)): #For sentence in list of sentences\n",
    "        for j in range(len(mod_train_tokens[i])): #for token in sentence\n",
    "            output = str(mod_train_tokens[i][j]) +'\\t' +str(mod_train_labels[i][j])+'\\n'\n",
    "            f.write(output)\n",
    "        f.write('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7808b17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('epi_gold_val-dz.tsv', \"w\") as f:\n",
    "    for i in range(len(mod_val_tokens)): #For sentence in list of sentences\n",
    "        for j in range(len(mod_val_tokens[i])): #for token in sentence\n",
    "            output = str(mod_val_tokens[i][j]) +'\\t' +str(mod_val_labels[i][j])+'\\n'\n",
    "            f.write(output)\n",
    "        f.write('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "15a4ac3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('epi_gold_test-dz.tsv', \"w\") as f:\n",
    "    for i in range(len(mod_test_tokens)): #For sentence in list of sentences\n",
    "        for j in range(len(mod_test_tokens[i])): #for token in sentence\n",
    "            output = str(mod_test_tokens[i][j]) +'\\t' +str(mod_test_labels[i][j])+'\\n'\n",
    "            f.write(output)\n",
    "        f.write('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39af506e",
   "metadata": {},
   "source": [
    "### Removing DIS, ABRV, & ETHN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cde052f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_disease_ethnicity(tokens,labels):\n",
    "    if len(tokens)!=len(labels):\n",
    "        raise IndexError(\"Num Sentences {} and Num Sentence Labels {} Mismatch\".format(len(tokens),len(labels)))\n",
    "    \n",
    "    for i in tqdm(range(len(tokens))):\n",
    "        if len(tokens[i])!=len(labels[i]):\n",
    "            raise IndexError(\"Sentence Length {} and Label Length {} Mismatch\".format(len(tokens[i]),len(labels[i])))\n",
    "        for j in range(len(labels[i])):\n",
    "            if labels[i][j] in {'B-ABRV','I-ABRV','B-DIS','I-DIS','B-ETHN','I-ETHN'}:\n",
    "                labels[i][j]='O'\n",
    "    \n",
    "    return tokens, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b2e1052b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4426/4426 [00:00<00:00, 236216.13it/s]\n"
     ]
    }
   ],
   "source": [
    "mod_train_tokens, mod_train_labels = remove_disease_ethnicity(epi_train_tokens,epi_train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1264c69c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1206/1206 [00:00<00:00, 234409.87it/s]\n"
     ]
    }
   ],
   "source": [
    "mod_val_tokens, mod_val_labels = remove_disease_ethnicity(epi_val_tokens,epi_val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "742e3ea9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 537/537 [00:00<00:00, 231579.40it/s]\n"
     ]
    }
   ],
   "source": [
    "mod_test_tokens, mod_test_labels = remove_disease_ethnicity(epi_test_tokens,epi_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "33085ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_train_val_tokens = mod_train_tokens+mod_val_tokens\n",
    "mod_train_val_labels = mod_train_labels+mod_val_labels\n",
    "\n",
    "with open('epi_gold_train_val-dz.tsv', \"w\") as f:\n",
    "    for i in range(len(mod_train_val_tokens)): #For sentence in list of sentences\n",
    "        for j in range(len(mod_train_val_tokens[i])): #for token in sentence\n",
    "            output = str(mod_train_val_tokens[i][j]) +'\\t' +str(mod_train_val_labels[i][j])+'\\n'\n",
    "            f.write(output)\n",
    "        f.write('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1290c3f",
   "metadata": {},
   "source": [
    "#### Saving the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a82c8b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('epi_gold_train-dz-ethn.tsv', \"w\") as f:\n",
    "    for i in range(len(mod_train_tokens)): #For sentence in list of sentences\n",
    "        for j in range(len(mod_train_tokens[i])): #for token in sentence\n",
    "            output = str(mod_train_tokens[i][j]) +'\\t' +str(mod_train_labels[i][j])+'\\n'\n",
    "            f.write(output)\n",
    "        f.write('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ebceddef",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('epi_gold_val-dz-ethn.tsv', \"w\") as f:\n",
    "    for i in range(len(mod_val_tokens)): #For sentence in list of sentences\n",
    "        for j in range(len(mod_val_tokens[i])): #for token in sentence\n",
    "            output = str(mod_val_tokens[i][j]) +'\\t' +str(mod_val_labels[i][j])+'\\n'\n",
    "            f.write(output)\n",
    "        f.write('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "754d61f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('epi_gold_test-dz-ethn.tsv', \"w\") as f:\n",
    "    for i in range(len(mod_test_tokens)): #For sentence in list of sentences\n",
    "        for j in range(len(mod_test_tokens[i])): #for token in sentence\n",
    "            output = str(mod_test_tokens[i][j]) +'\\t' +str(mod_test_labels[i][j])+'\\n'\n",
    "            f.write(output)\n",
    "        f.write('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "074c28fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_train_val_tokens = mod_train_tokens+mod_val_tokens\n",
    "mod_train_val_labels = mod_train_labels+mod_val_labels\n",
    "\n",
    "with open('epi_gold_train_val-dz-ethn.tsv', \"w\") as f:\n",
    "    for i in range(len(mod_train_val_tokens)): #For sentence in list of sentences\n",
    "        for j in range(len(mod_train_val_tokens[i])): #for token in sentence\n",
    "            output = str(mod_train_val_tokens[i][j]) +'\\t' +str(mod_train_val_labels[i][j])+'\\n'\n",
    "            f.write(output)\n",
    "        f.write('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38d0a8a",
   "metadata": {},
   "source": [
    "### Adding Location (CoNLL++)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2160c6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset conllpp (/home/wzkariampuzha/.cache/huggingface/datasets/conllpp/conllpp/1.0.0/04f15f257dff3fe0fb36e049b73d51ecdf382698682f5e590b7fb13898206ba2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0cb7d5df65a426182ca792ef1bff796",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install datasets\n",
    "from datasets import load_dataset\n",
    "coNLL = load_dataset(\"conllpp\")\n",
    "coNLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "051b78fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NER_tag '5' is B-LOC, '6' is I-LOC\n",
    "#Get numbers on the amount of location \n",
    "\n",
    "def read_loc_dataset(dataset):\n",
    "    token_docs = []\n",
    "    tag_docs = []\n",
    "    for sentence in dataset:\n",
    "        #Only add sentences that actually have location tags (i.e. meaningfully annotated sentences)\n",
    "        if (5 in sentence['ner_tags'] or 6 in sentence['ner_tags']):\n",
    "            tags = []\n",
    "            #Only keep location tags\n",
    "            for tag in sentence['ner_tags']:\n",
    "                label = 'O'\n",
    "                if tag ==5:\n",
    "                    label = 'B-LOC'\n",
    "                if tag == 6:\n",
    "                    label = 'I-LOC'\n",
    "                tags.append(label)\n",
    "            \n",
    "            #Raise error if mismatch\n",
    "            if len(sentence['tokens']) != len(tags):\n",
    "                print('mismatch')\n",
    "                print(sentence['tokens'])\n",
    "                print(tags)\n",
    "            \n",
    "            token_docs.append(sentence['tokens'])\n",
    "            tag_docs.append(tags)\n",
    "        \n",
    "    return token_docs, tag_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cb39ad1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts_loc, train_tags_loc = read_loc_dataset(coNLL[\"train\"])\n",
    "val_texts_loc, val_tags_loc = read_loc_dataset(coNLL[\"validation\"])\n",
    "test_texts_loc, test_tags_loc = read_loc_dataset(coNLL[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2f841eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BRUSSELS B-LOC\n",
      "1996-08-22 O\n",
      "\n",
      "Germany B-LOC\n",
      "'s O\n",
      "representative O\n",
      "to O\n",
      "the O\n",
      "European O\n",
      "Union O\n",
      "'s O\n",
      "veterinary O\n",
      "committee O\n",
      "Werner O\n",
      "Zwingmann O\n",
      "said O\n",
      "on O\n",
      "Wednesday O\n",
      "consumers O\n",
      "should O\n",
      "buy O\n",
      "sheepmeat O\n",
      "from O\n",
      "countries O\n",
      "other O\n",
      "than O\n",
      "Britain B-LOC\n",
      "until O\n",
      "the O\n",
      "scientific O\n",
      "advice O\n",
      "was O\n",
      "clearer O\n",
      ". O\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Show the data\n",
    "for i in range(2): #for sentence in abstract\n",
    "    for j in range(len((train_texts_loc[i]))): #for token in sentence\n",
    "        print(train_texts_loc[i][j], train_tags_loc[i][j])\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a6502f",
   "metadata": {},
   "source": [
    "#### Saving the modified train set + location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "10915f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12135 12135\n"
     ]
    }
   ],
   "source": [
    "#Add extra location data to the modified (gold-dz-ethn) data\n",
    "train_texts = mod_train_tokens + train_texts_loc + val_texts_loc + test_texts_loc\n",
    "train_tags = mod_train_labels + train_tags_loc + val_tags_loc + test_tags_loc\n",
    "print(len(train_texts),len(train_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "76f97bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12135/12135 [00:00<00:00, 41823.51it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('epi_gold_train++loc-dz-ethn.tsv', \"w\") as f:\n",
    "    for i in tqdm(range(len(train_texts))): #For sentence in abstract\n",
    "        for j in range(len(train_texts[i])): #for token in sentence\n",
    "            output = str(train_texts[i][j]) +'\\t' +str(train_tags[i][j])+'\\n'\n",
    "            f.write(output)\n",
    "        f.write('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb77bc4",
   "metadata": {},
   "source": [
    "#### Saving the modified train+val set + location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2ebdaebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13341/13341 [00:00<00:00, 40978.85it/s]\n"
     ]
    }
   ],
   "source": [
    "train_texts+=mod_val_tokens\n",
    "train_tags+=mod_val_labels\n",
    "with open('epi_gold_train_val++loc-dz-ethn.tsv', \"w\") as f:\n",
    "    for i in tqdm(range(len(train_texts))): #For sentence in abstract\n",
    "        for j in range(len(train_texts[i])): #for token in sentence\n",
    "            output = str(train_texts[i][j]) +'\\t' +str(train_tags[i][j])+'\\n'\n",
    "            f.write(output)\n",
    "        f.write('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3662d284",
   "metadata": {},
   "source": [
    "### Converting the inputs to identify sentences (multilabel sentence classification)\n",
    "Text classification at the sentence level if the sentence has B-STAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "35ffdcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input will be 2 lists of lists, Output will be list of tuple size 9.\n",
    "def tokens2sents(tokens,sentence_labels):\n",
    "    if len(tokens)!=len(sentence_labels):\n",
    "        raise IndexError(\"Num Sentences {} and Num Sentence sentence_labels {} Mismatch\".format(len(tokens),len(sentence_labels)))\n",
    "    output = []\n",
    "    \n",
    "    for i in tqdm(range(len(tokens))):\n",
    "        if len(tokens[i])!=len(sentence_labels[i]):\n",
    "            raise IndexError(\"Sentence Length {} and Label Length {} Mismatch\".format(len(tokens[i]),len(sentence_labels[i])))\n",
    "        sentence = ' '.join(tokens[i])\n",
    "        stat_label = int('B-STAT' in sentence_labels[i] or 'I-STAT' in sentence_labels[i]) #boolean evaluates to 1 if True, 0 if False\n",
    "        epi_label = int('B-EPI' in sentence_labels[i] or 'I-EPI' in sentence_labels[i])\n",
    "        dis_label = int('B-DIS' in sentence_labels[i] or 'I-DIS' in sentence_labels[i])\n",
    "        abrv_label = int('B-ABRV' in sentence_labels[i] or 'I-ABRV' in sentence_labels[i])\n",
    "        loc_label = int('B-LOC' in sentence_labels[i] or 'I-LOC' in sentence_labels[i])\n",
    "        ethn_label = int('B-ETHN' in sentence_labels[i] or 'I-ETHN' in sentence_labels[i])\n",
    "        date_label = int('B-DATE' in sentence_labels[i] or 'I-DATE' in sentence_labels[i])\n",
    "        sex_label = int('B-SEX' in sentence_labels[i] or 'I-SEX' in sentence_labels[i])\n",
    "        output.append((str(sentence),str(stat_label),str(epi_label),str(dis_label),str(abrv_label),str(loc_label),str(ethn_label),str(date_label),str(sex_label)))\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c3457623",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4426/4426 [00:00<00:00, 75913.30it/s]\n"
     ]
    }
   ],
   "source": [
    "train_output = tokens2sents(epi_train_tokens,epi_train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9344d50a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1206/1206 [00:00<00:00, 77887.57it/s]\n"
     ]
    }
   ],
   "source": [
    "val_output = tokens2sents(epi_val_tokens,epi_val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "33943c8f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 537/537 [00:00<00:00, 74538.88it/s]\n"
     ]
    }
   ],
   "source": [
    "test_output = tokens2sents(epi_test_tokens,epi_test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc3ff08",
   "metadata": {},
   "source": [
    "#### Saving the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "375a1261",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('epi_gold_train_sents.tsv', \"w\") as f:\n",
    "    for i in range(len(train_output)): #For sentence in list of sentences\n",
    "        output = '\\t'.join(train_output[i])+'\\n'\n",
    "        f.write(output)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4499c6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('epi_gold_val_sents.tsv', \"w\") as f:\n",
    "    for i in range(len(val_output)): #For sentence in list of sentences\n",
    "        output = '\\t'.join(val_output[i])+'\\n'\n",
    "        f.write(output)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2d2f4d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('epi_gold_test_sents.tsv', \"w\") as f:\n",
    "    for i in range(len(test_output)): #For sentence in list of sentences\n",
    "        output = '\\t'.join(test_output[i])+'\\n'\n",
    "        f.write(output)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

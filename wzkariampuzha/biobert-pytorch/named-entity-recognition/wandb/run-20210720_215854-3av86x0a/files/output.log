

























































































































































































































































 42% 250/600 [36:45<52:30,  9.00s/it]Saving model checkpoint to ./output/Location/checkpoint-250
Configuration saved in ./output/Location/checkpoint-250/config.json
Model weights saved in ./output/Location/checkpoint-250/pytorch_model.bin
























































































































































































































































 83% 500/600 [1:13:39<17:27, 10.47s/it]Saving model checkpoint to ./output/Location/checkpoint-500
Configuration saved in ./output/Location/checkpoint-500/config.json
Model weights saved in ./output/Location/checkpoint-500/pytorch_model.bin


































































































100% 600/600 [1:28:13<00:00,  7.74s/it]
Training completed. Do not forget to share your model on huggingface.co/models =)
100% 600/600 [1:28:13<00:00,  8.82s/it]
Saving model checkpoint to ./output/Location
Configuration saved in ./output/Location/config.json
Model weights saved in ./output/Location/pytorch_model.bin
tokenizer config file saved in ./output/Location/tokenizer_config.json
Special tokens file saved in ./output/Location/special_tokens_map.json
07/20/2021 23:27:11 - INFO - __main__ -   *** Evaluate ***
***** Running Evaluation *****
  Num examples = 89
  Batch size = 8


100% 12/12 [00:05<00:00,  2.01it/s]
07/20/2021 23:27:18 - INFO - __main__ -   ***** Eval results *****
07/20/2021 23:27:18 - INFO - __main__ -     eval_loss = 0.09528274834156036
07/20/2021 23:27:18 - INFO - __main__ -     eval_precision = 0.8922345483359746
07/20/2021 23:27:18 - INFO - __main__ -     eval_recall = 0.929042904290429
07/20/2021 23:27:18 - INFO - __main__ -     eval_f1 = 0.9102667744543249
07/20/2021 23:27:18 - INFO - __main__ -     eval_runtime = 6.6135
07/20/2021 23:27:18 - INFO - __main__ -     eval_samples_per_second = 13.457
07/20/2021 23:27:18 - INFO - __main__ -     eval_steps_per_second = 1.814
07/20/2021 23:27:18 - INFO - __main__ -     epoch = 50.0
07/20/2021 23:27:18 - INFO - filelock -   Lock 139851724843760 acquired on ../datasets/NER/Location/cached_test_BertTokenizer_192.lock
07/20/2021 23:27:18 - INFO - utils_ner -   Loading features from cached file ../datasets/NER/Location/cached_test_BertTokenizer_192
07/20/2021 23:27:18 - INFO - filelock -   Lock 139851724843760 released on ../datasets/NER/Location/cached_test_BertTokenizer_192.lock
***** Running Prediction *****
  Num examples = 94
  Batch size = 8


100% 12/12 [00:06<00:00,  1.82it/s]07/20/2021 23:27:25 - INFO - __main__ -   ***** Test results *****
07/20/2021 23:27:25 - INFO - __main__ -     test_loss = 0.09133969992399216
07/20/2021 23:27:25 - INFO - __main__ -     test_precision = 0.9111424541607899
07/20/2021 23:27:25 - INFO - __main__ -     test_recall = 0.9176136363636364
07/20/2021 23:27:25 - INFO - __main__ -     test_f1 = 0.9143665958952584
07/20/2021 23:27:25 - INFO - __main__ -     test_runtime = 7.3604
07/20/2021 23:27:25 - INFO - __main__ -     test_samples_per_second = 12.771
07/20/2021 23:27:25 - INFO - __main__ -     test_steps_per_second = 1.63
{'loss': 0.0114, 'learning_rate': 8.333333333333334e-06, 'epoch': 41.67}
{'train_runtime': 5295.9857, 'train_samples_per_second': 3.493, 'train_steps_per_second': 0.113, 'train_loss': 0.009529552524909377, 'epoch': 50.0}
100% 12/12 [00:07<00:00,  1.67it/s]